{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPaWhvqW8NWI",
        "outputId": "494a2609-fa0b-4d95-e35d-37c3b4e33e90"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd /content\n",
        "!rm -rf /content/alphageometry\n",
        "!git clone --depth 1 https://github.com/haunt98/alphageometry.git\n",
        "%cd /content/alphageometry"
      ],
      "metadata": {
        "id": "BGyLJ6HO8PWR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install -r requirements.in"
      ],
      "metadata": {
        "id": "BKVao0qG8VIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache models from https://bit.ly/alphageometry\n",
        "!mkdir /content/alphageometry/ag_ckpt_vocab\n",
        "!cp /content/gdrive/MyDrive/alphageometry/* /content/alphageometry/ag_ckpt_vocab/\n",
        "!ls /content/alphageometry/ag_ckpt_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_vQj9rW_LJ0",
        "outputId": "4829e709-49a3-4ba5-87a7-df7a0fe6a631"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint_10999999  geometry.757.model  geometry.757.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%env MELIAD_PATH=meliad_lib/meliad\n",
        "!mkdir -p $MELIAD_PATH\n",
        "!git clone --depth 1 https://github.com/google-research/meliad $MELIAD_PATH"
      ],
      "metadata": {
        "id": "eVsjbt_vCacX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/a/57240319\n",
        "import os\n",
        "\n",
        "print('Before PYTHONPATH', os.environ['PYTHONPATH'])\n",
        "# os.environ['PYTHONPATH'] = '/env/python'\n",
        "\n",
        "if '/content/alphageometry/meliad_lib/meliad' not in os.environ['PYTHONPATH']:\n",
        "  os.environ['PYTHONPATH'] += ':/content/alphageometry/meliad_lib/meliad'\n",
        "\n",
        "print('After PYTHONPATH', os.environ['PYTHONPATH'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN7k3Tp0Ckr3",
        "outputId": "ea087279-2740-4ac0-dae0-7349caa050a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before PYTHONPATH /env/python:$MELIAD_PATH\n",
            "After PYTHONPATH /env/python:/content/alphageometry/meliad_lib/meliad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m alphageometry \\\n",
        "  --alsologtostderr \\\n",
        "  --problems_file=examples.txt \\\n",
        "  --problem_name=orthocenter \\\n",
        "  --mode=alphageometry \\\n",
        "  --defs_file=defs.txt \\\n",
        "  --rules_file=rules.txt \\\n",
        "  --beam_size=2 \\\n",
        "  --search_depth=2 \\\n",
        "  --ckpt_path=ag_ckpt_vocab \\\n",
        "  --vocab_path=ag_ckpt_vocab/geometry.757.model \\\n",
        "  --gin_search_paths=$MELIAD_PATH/transformer/configs \\\n",
        "  --gin_file=base_htrans.gin \\\n",
        "  --gin_file=size/medium_150M.gin \\\n",
        "  --gin_file=options/positions_t5.gin \\\n",
        "  --gin_file=options/lr_cosine_decay.gin \\\n",
        "  --gin_file=options/seq_1024_nocache.gin \\\n",
        "  --gin_file=geometry_150M_generate.gin \\\n",
        "  --gin_param=DecoderOnlyLanguageModelGenerate.output_token_losses=True \\\n",
        "  --gin_param=TransformerTaskConfig.batch_size=2 \\\n",
        "  --gin_param=TransformerTaskConfig.sequence_length=128 \\\n",
        "  --gin_param=Trainer.restore_state_variables=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvVexyo7Dvbp",
        "outputId": "48dfb775-e6b3-4e76-f3dc-1feb3b2bd33a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "2024-04-04 16:59:52.706202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "I0404 16:59:56.515090 136134801375232 inference_utils.py:69] Parsing gin configuration.\n",
            "I0404 16:59:56.515360 136134801375232 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs\n",
            "I0404 16:59:56.515820 136134801375232 inference_utils.py:74] Loading Gin config file base_htrans.gin\n",
            "I0404 16:59:56.515911 136134801375232 inference_utils.py:74] Loading Gin config file size/medium_150M.gin\n",
            "I0404 16:59:56.515982 136134801375232 inference_utils.py:74] Loading Gin config file options/positions_t5.gin\n",
            "I0404 16:59:56.516048 136134801375232 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin\n",
            "I0404 16:59:56.516132 136134801375232 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin\n",
            "I0404 16:59:56.516208 136134801375232 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin\n",
            "I0404 16:59:56.516284 136134801375232 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True\n",
            "I0404 16:59:56.516346 136134801375232 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2\n",
            "I0404 16:59:56.516407 136134801375232 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128\n",
            "I0404 16:59:56.516465 136134801375232 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False\n",
            "I0404 16:59:56.516577 136134801375232 resource_reader.py:50] system_path_file_exists:base_htrans.gin\n",
            "E0404 16:59:56.516853 136134801375232 resource_reader.py:55] Path not found: base_htrans.gin\n",
            "I0404 16:59:56.517267 136134801375232 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin\n",
            "E0404 16:59:56.517539 136134801375232 resource_reader.py:55] Path not found: trainer_configuration.gin\n",
            "I0404 16:59:56.528015 136134801375232 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin\n",
            "E0404 16:59:56.528437 136134801375232 resource_reader.py:55] Path not found: size/medium_150M.gin\n",
            "I0404 16:59:56.529269 136134801375232 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin\n",
            "E0404 16:59:56.529557 136134801375232 resource_reader.py:55] Path not found: options/positions_t5.gin\n",
            "I0404 16:59:56.530090 136134801375232 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin\n",
            "E0404 16:59:56.530370 136134801375232 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin\n",
            "I0404 16:59:56.531140 136134801375232 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin\n",
            "E0404 16:59:56.531381 136134801375232 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin\n",
            "I0404 16:59:56.783278 136134801375232 training_loop.py:334] ==== Training loop: initializing model ====\n",
            "I0404 16:59:56.786319 136134801375232 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.\n",
            "I0404 16:59:56.786527 136134801375232 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'\n",
            "I0404 16:59:56.786635 136134801375232 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
            "I0404 16:59:56.786700 136134801375232 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
            "I0404 16:59:56.787602 136134801375232 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
            "I0404 16:59:56.787773 136134801375232 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
            "W0404 16:59:56.787882 136134801375232 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "I0404 16:59:56.788001 136134801375232 training_loop.py:335] Process 0 of 1\n",
            "I0404 16:59:56.788089 136134801375232 training_loop.py:336] Local device count = 1\n",
            "I0404 16:59:56.788279 136134801375232 training_loop.py:337] Number of replicas = 1\n",
            "I0404 16:59:56.788352 136134801375232 training_loop.py:339] Using random number seed 42\n",
            "I0404 16:59:57.321677 136134801375232 training_loop.py:359] Initializing the model.\n",
            "I0404 16:59:57.599282 136134801375232 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.600032 136134801375232 decoder_stack.py:316] dstack: scanning over 1 windows.\n",
            "I0404 16:59:57.600234 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.600347 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.600460 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.600557 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.600651 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.600746 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.600839 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.600934 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.601028 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.601131 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.601231 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.601324 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0404 16:59:57.601410 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:57.601496 136134801375232 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0404 16:59:57.601760 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:57.601856 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:57.601927 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:57.606053 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.621943 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:57.663735 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.665056 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:57.676655 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:57.714203 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:57.714449 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:57.714545 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:57.714631 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.714822 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.717501 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.717809 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.719873 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.727271 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.743670 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.746475 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.746774 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:57.746858 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:57.746985 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.747533 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:57.748298 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:57.748449 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:57.758436 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.758822 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:57.766566 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.766892 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:57.767757 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:57.802012 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:57.831699 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.832112 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:57.832901 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.833195 136134801375232 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0404 16:59:57.833483 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:57.833605 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:57.833690 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:57.838304 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.845339 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:57.878478 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.879713 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:57.890402 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:57.905368 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:57.905628 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:57.905746 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:57.905848 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.906074 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.907022 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.907311 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.908264 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.911201 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.922774 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.923977 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.924322 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:57.924431 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:57.924576 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.925180 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:57.925977 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:57.926130 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:57.935766 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.936170 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:57.942361 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.942629 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:57.943279 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:57.952927 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:57.961737 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.962145 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:57.963068 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.963401 136134801375232 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0404 16:59:57.963695 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:57.963818 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:57.963896 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:57.968156 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:57.977257 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.010938 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.014391 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.022176 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.039071 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.039371 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.039496 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.039586 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.039793 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.040886 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.041159 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.042139 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.044775 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.051698 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.052763 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.053036 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.053138 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.053265 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.053778 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.054539 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.054666 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.063071 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.063482 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.072057 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.072476 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.073454 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:58.083873 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.092589 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.093042 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.093958 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.094344 136134801375232 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0404 16:59:58.094667 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:58.094793 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:58.094874 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:58.100395 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.106815 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.132899 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.134321 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.141619 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.162518 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.162768 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.162886 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.162977 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.163222 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.164225 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.164891 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.165922 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.167959 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.173531 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.174587 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.174849 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.174932 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.175052 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.175558 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.176275 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.176399 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.185184 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.185582 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.193799 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.194186 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.195126 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:58.205656 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.214154 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.214708 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.215481 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.215762 136134801375232 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0404 16:59:58.216013 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:58.216116 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:58.216190 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:58.219964 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.225803 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.253886 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.255446 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.260878 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.275616 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.275895 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.275997 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.276089 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.276322 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.277252 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.277544 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.278500 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.280854 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.286814 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.287839 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.288114 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.288197 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.288323 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.288828 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.289680 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.289823 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.298139 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.298542 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.304980 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.305340 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.306179 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:58.315146 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.323385 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.323801 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.324657 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.324967 136134801375232 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0404 16:59:58.325302 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:58.325431 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:58.325520 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:58.329641 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.337561 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.367768 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.369206 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.377127 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.391002 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.391279 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.391397 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.391491 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.391713 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.392610 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.392855 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.393704 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.395857 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.401563 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.402583 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.402858 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.402941 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.403064 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.403568 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.404327 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.404465 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.412429 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.412812 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.419195 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.419545 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.420449 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:58.429510 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.448910 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.449304 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.450895 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.451220 136134801375232 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0404 16:59:58.451493 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:58.451583 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:58.451651 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:58.458122 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.468107 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.495950 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.497202 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.503271 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.515360 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.515604 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.515701 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.515788 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.516002 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.516876 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.517144 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.517973 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.520105 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.525783 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.526746 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.526982 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.527062 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.527202 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.527719 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.528454 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.528578 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.536631 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.536981 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.543391 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.543710 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.544544 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:58.553204 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.561016 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.561404 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.562089 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.562418 136134801375232 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0404 16:59:58.562689 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:58.562789 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:58.562865 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:58.567246 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.574645 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.600560 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.601907 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.609587 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.621665 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.621898 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.622001 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.622088 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.622293 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.623285 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.623560 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.624474 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.626647 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.632277 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.633298 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.633558 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.633638 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.633762 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.634281 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.635262 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.635440 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.644570 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.644982 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.651314 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.651659 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.652503 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:58.661677 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.669815 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.670212 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.671206 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.671549 136134801375232 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0404 16:59:58.671838 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:58.671941 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:58.672014 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:58.676184 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.682533 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.705222 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.706444 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.712529 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.723855 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.724110 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.724213 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.724326 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.724527 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.725358 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.725620 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.726479 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.728641 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.734262 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.735252 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.735524 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.735603 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.735726 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.736223 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.742127 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.742328 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.760612 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.761007 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.768505 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.768839 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.769673 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:58.780051 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.788525 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.788909 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.789660 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.789939 136134801375232 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0404 16:59:58.790209 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:58.790308 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:58.790378 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:58.794280 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.800405 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.824684 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.825963 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.832324 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.845151 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.845393 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.845485 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.845570 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.845754 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.846575 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.846828 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.847679 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.849866 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.855463 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.856504 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.856766 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.856846 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.856979 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.857507 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.858248 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.858379 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.867942 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.868438 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.877765 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.878160 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.879030 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:58.888314 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.900477 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.901196 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.902132 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.902435 136134801375232 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0404 16:59:58.902711 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:58.902816 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:58.902885 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:58.906978 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.912892 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:58.942912 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.944344 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:58.951764 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:58.965770 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:58.966013 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:58.966128 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:58.966220 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.966408 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.967284 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.967552 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.968602 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.970940 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.976515 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.977512 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.977867 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:58.977997 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:58.978196 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.978734 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:58.979476 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:58.979603 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:58.987726 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.988122 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:58.994640 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:58.994982 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:58.995801 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:59.008620 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.016442 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.016736 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.017487 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.017822 136134801375232 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0404 16:59:59.018141 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:59.018279 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:59.018355 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:59.022415 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.029449 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:59.064670 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.065908 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:59.072152 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 16:59:59.083544 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:59.083797 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:59.083888 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:59.083969 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.084172 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.085044 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.085313 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.086180 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.088373 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.094004 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.095025 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.096062 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:59.096196 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:59.096329 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.096923 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:59.097713 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:59.097858 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.105975 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.106399 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.112819 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.113179 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:59.114003 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:59.123725 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.131758 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.132152 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.132853 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.133585 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.133738 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.133844 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.133929 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134010 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134089 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134187 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134266 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134344 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134421 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134496 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134571 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0404 16:59:59.134644 136134801375232 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0404 16:59:59.144653 136134801375232 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 16:59:59.328753 136134801375232 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.329140 136134801375232 decoder_stack.py:333] dstack: autoregressive generator.\n",
            "I0404 16:59:59.329274 136134801375232 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0404 16:59:59.329522 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:59.329618 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:59.329685 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:59.329808 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.336456 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:59.374411 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.375623 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:59.381821 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 16:59:59.424259 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:59.424541 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:59.424628 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:59.424712 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.424913 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.427222 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.427536 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.429675 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.435352 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.450519 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.453190 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.453529 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:59.453615 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:59.453738 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.454245 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:59.454540 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:59.454636 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.463394 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.463758 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.469637 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.469971 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:59.470235 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:59.481362 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.490767 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.491167 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.491905 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.492198 136134801375232 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0404 16:59:59.492459 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:59.492552 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:59.492623 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:59.492733 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.498399 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:59.529658 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.531592 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:59.538729 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 16:59:59.577926 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:59.578156 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:59.578270 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:59.578357 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.578578 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.579521 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.579782 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.580824 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.584879 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.591873 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.593007 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.593321 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:59.593429 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:59.593567 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.594198 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:59.594523 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:59.594647 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.602592 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.602996 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.609140 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.609475 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:59.609743 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:59.623826 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.634128 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.634536 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.635509 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.635905 136134801375232 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0404 16:59:59.636265 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:59.636400 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:59.636497 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:59.636635 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.651751 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:59.687659 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.689064 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:59.698818 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 16:59:59.734718 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:59.734977 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:59.735072 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:59.735207 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.735424 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.736322 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.736553 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.737462 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.739429 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.745532 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.746578 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.746846 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:59.746929 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:59.747062 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.747654 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:59.747959 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:59.748510 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.758527 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.758875 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.764960 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.765305 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:59.765567 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:59.779762 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.790160 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.790472 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.791009 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.791260 136134801375232 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0404 16:59:59.791471 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:59.791569 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:59.791650 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:59.791773 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.798024 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:59.825186 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.826401 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:59.832530 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 16:59:59.864035 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 16:59:59.864326 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 16:59:59.864435 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 16:59:59.864531 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.864744 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.865796 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.866074 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.867053 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.869070 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.876190 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.877408 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.877736 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 16:59:59.877835 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 16:59:59.877975 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.878602 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 16:59:59.878942 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 16:59:59.879071 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.892256 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.892688 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.900306 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.900678 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 16:59:59.901017 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 16:59:59.916272 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 16:59:59.927621 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.928036 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 16:59:59.928967 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.929287 136134801375232 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0404 16:59:59.929590 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 16:59:59.929707 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 16:59:59.929791 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 16:59:59.929920 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.936494 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 16:59:59.973192 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 16:59:59.980050 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 16:59:59.987729 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:00.042608 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:00.042850 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:00.042945 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:00.043036 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.043299 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.045049 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.045311 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.046041 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.048057 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.054212 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.055392 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.055715 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:00.055804 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:00.055937 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.056545 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:00.056896 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:00.057010 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.066403 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.066974 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.073853 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.074250 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:00.074606 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:00.085605 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.098216 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.098685 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.099633 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.099984 136134801375232 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0404 17:00:00.100330 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:00.100462 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:00.100545 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:00.100670 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.107393 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:00.136023 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.137536 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:00.144757 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:00.183380 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:00.183623 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:00.183746 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:00.183835 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.184049 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.184991 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.185236 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.186161 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.189337 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.198913 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.200020 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.200325 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:00.200420 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:00.200570 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.201134 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:00.201458 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:00.201577 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.211894 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.212304 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.219019 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.219393 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:00.219716 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:00.231150 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.240719 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.241145 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.241966 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.242280 136134801375232 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0404 17:00:00.242612 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:00.242729 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:00.242815 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:00.242942 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.258000 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:00.290900 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.292411 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:00.299510 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:00.336620 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:00.336868 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:00.336991 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:00.337086 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.337350 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.338305 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.338564 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.339507 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.341540 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.349404 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.350506 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.350791 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:00.350885 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:00.351025 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.351645 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:00.351971 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:00.352118 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.362456 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.362859 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.370354 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.370737 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:00.371044 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:00.383339 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.393606 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.394054 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.394983 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.395334 136134801375232 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0404 17:00:00.395650 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:00.395792 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:00.395910 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:00.396046 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.402754 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:00.434088 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.435523 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:00.445346 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:00.488605 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:00.488876 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:00.488992 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:00.489125 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.489357 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.490351 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.490644 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.491650 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.493817 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.501473 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.502923 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.503300 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:00.503447 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:00.503630 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.504455 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:00.504797 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:00.504922 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.515156 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.515600 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.523729 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.524150 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:00.524484 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:00.536155 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.554177 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.554588 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.555537 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.557586 136134801375232 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0404 17:00:00.558007 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:00.558169 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:00.558260 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:00.558400 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.566286 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:00.599819 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.601661 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:00.609521 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:00.651220 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:00.651673 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:00.651843 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:00.651964 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.652265 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.653470 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.653902 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.655174 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.657501 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.664900 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.666086 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.666448 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:00.666560 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:00.666703 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.667402 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:00.667788 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:00.667947 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.679661 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.680077 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.687357 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.687728 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:00.688035 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:00.699309 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.707725 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.708024 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.708660 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.708910 136134801375232 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0404 17:00:00.709123 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:00.709228 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:00.709280 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:00.709376 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.713617 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:00.732923 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.733845 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:00.738478 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:00.769115 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:00.769331 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:00.769416 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:00.769474 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.769638 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.770389 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.770601 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.771319 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.772821 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.777144 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.777940 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.778166 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:00.778228 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:00.778316 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.778700 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:00.778932 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:00.779004 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.785281 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.785577 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.789811 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.790113 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:00.790304 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:00.796967 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.802950 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.803273 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.803797 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.804014 136134801375232 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0404 17:00:00.804220 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:00.804321 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:00.804380 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:00.804463 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.808418 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:00.825754 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.826802 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:00.831213 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:00.859690 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:00.859911 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:00.860005 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:00.860087 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.860302 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.860980 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.861198 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.862018 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.863490 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.867701 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.868486 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.868673 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:00.868730 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:00.868821 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.869248 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:00.869532 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:00.869611 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.877733 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.878158 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.883542 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.883849 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:00.884073 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:00.892122 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.898717 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.899221 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.899868 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.900143 136134801375232 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0404 17:00:00.900388 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:00.900513 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:00.900588 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:00.900701 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.904935 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:00.924808 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.925723 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:00.930374 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:00.953517 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:00.953836 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:00.953921 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:00.953986 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.954177 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.954868 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.955076 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.955775 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.957156 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.961314 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.962135 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.962328 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:00.962383 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:00.962483 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.962849 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:00.963067 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:00.963159 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.969756 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.970066 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.975358 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.975648 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:00.975857 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:00.983360 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:00.990398 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.990726 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:00.991398 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:00.991681 136134801375232 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0404 17:00:00.997591 136134801375232 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:01.158539 136134801375232 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.158970 136134801375232 decoder_stack.py:333] dstack: autoregressive generator.\n",
            "I0404 17:00:01.159147 136134801375232 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0404 17:00:01.159476 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:01.159609 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:01.159698 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:01.159871 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.166145 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:01.187546 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.188501 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:01.193444 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:01.218413 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:01.218630 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:01.218699 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:01.218756 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.218896 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.219504 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.219702 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.220315 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.221617 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.225648 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.226791 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.226984 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:01.227045 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:01.227160 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.227549 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:01.227763 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:01.227834 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.233784 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.234084 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.238225 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.238468 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:01.238662 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:01.246237 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.253685 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.253998 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.254602 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.254847 136134801375232 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0404 17:00:01.255079 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:01.255183 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:01.255266 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:01.255392 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.259424 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:01.279682 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.280603 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:01.284995 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:01.306533 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:01.306745 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:01.306856 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:01.306942 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.307131 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.307807 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.307995 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.308666 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.310039 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.314191 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.314930 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.315141 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:01.315228 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:01.315357 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.315780 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:01.316020 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:01.316125 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.322324 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.322632 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.326863 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.327147 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:01.327399 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:01.334551 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.629689 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.630024 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.630683 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.630926 136134801375232 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0404 17:00:01.631624 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:01.631762 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:01.631851 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:01.631981 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.636176 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:01.654877 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.655773 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:01.660288 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:01.686144 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:01.686350 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:01.686466 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:01.686554 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.686733 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.687463 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.687670 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.688546 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.689914 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.694009 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.694788 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.695016 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:01.695110 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:01.695241 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.695830 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:01.696135 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:01.696241 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.702564 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.702871 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.707170 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.707445 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:01.707682 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:01.714513 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.720640 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.720971 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.721546 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.721863 136134801375232 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0404 17:00:01.722139 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:01.722246 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:01.722326 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:01.722444 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.726586 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:01.747159 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.748824 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:01.754701 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:01.778856 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:01.779082 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:01.779216 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:01.779311 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.779489 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.780213 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.780402 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.781248 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.782575 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.786478 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.787245 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.787438 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:01.787813 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:01.787989 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.788475 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:01.788737 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:01.788847 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.795189 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.795560 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.799863 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.800145 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:01.800377 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:01.807147 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.813235 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.813552 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.814152 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.814404 136134801375232 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0404 17:00:01.814637 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:01.814728 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:01.814809 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:01.814924 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.818946 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:01.836069 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.836979 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:01.841116 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:01.868912 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:01.869127 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:01.869230 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:01.869316 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.869488 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.870190 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.870372 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.871043 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.872391 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.877284 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.878112 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.878330 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:01.878419 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:01.878546 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.878975 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:01.879247 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:01.879344 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.887196 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.887506 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.891727 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.891996 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:01.892198 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:01.898524 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.904401 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.904700 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.905218 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.905464 136134801375232 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0404 17:00:01.905640 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:01.905704 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:01.905751 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:01.905826 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.909913 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:01.931090 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.931985 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:01.936338 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:01.959707 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:01.959981 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:01.960177 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:01.960300 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.960514 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.961425 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.961653 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.962639 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.965392 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.972633 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.973649 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.973898 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:01.973987 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:01.974304 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.974907 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:01.975219 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:01.975326 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:01.981899 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.982233 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:01.986604 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:01.986876 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:01.987129 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:01.994413 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.000966 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.001306 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.001901 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.002158 136134801375232 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0404 17:00:02.002388 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:02.002490 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:02.002564 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:02.002677 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.006896 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:02.024557 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.025435 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:02.030206 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:02.053503 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:02.053722 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:02.053851 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:02.053954 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.054191 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.055247 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.055632 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.056403 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.057764 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.061822 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.062703 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.062961 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:02.063063 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:02.063206 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.063755 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:02.064035 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:02.064151 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.079112 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.079495 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.086579 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.086945 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:02.087284 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:02.097926 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.104173 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.104529 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.105058 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.105323 136134801375232 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0404 17:00:02.105536 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:02.105619 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:02.105688 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:02.105795 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.110955 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:02.128756 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.129647 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:02.135896 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:02.169584 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:02.169804 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:02.169900 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:02.169984 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.170189 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.170897 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.171082 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.171768 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.173151 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.177919 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.179004 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.179267 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:02.179360 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:02.179597 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.180154 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:02.180434 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:02.180655 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.187503 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.187802 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.192068 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.192353 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:02.192594 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:02.199344 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.205256 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.205568 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.206131 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.206362 136134801375232 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0404 17:00:02.206584 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:02.206669 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:02.206743 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:02.206857 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.210833 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:02.228145 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.229032 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:02.233414 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:02.257794 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:02.257986 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:02.258082 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:02.258204 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.258390 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.259046 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.259242 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.259879 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.261201 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.266145 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.267025 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.267271 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:02.267368 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:02.267499 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.267955 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:02.268240 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:02.268342 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.274386 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.274687 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.279705 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.279986 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:02.280268 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:02.287369 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.294405 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.294896 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.295564 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.295829 136134801375232 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0404 17:00:02.296110 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:02.296205 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:02.296280 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:02.296407 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.300628 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:02.317836 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.318735 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:02.322780 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:02.346776 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:02.346988 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:02.347112 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:02.347199 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.347382 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.348039 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.348241 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.348901 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.350244 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.354205 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.354920 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.355118 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:02.355204 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:02.355329 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.355779 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:02.356014 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:02.356119 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.366049 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.366577 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.374422 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.374702 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:02.374936 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:02.382747 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.389838 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.390176 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.390773 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.391021 136134801375232 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0404 17:00:02.391268 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:02.391360 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:02.391434 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:02.391551 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.395692 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:02.416819 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.417776 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:02.423280 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:02.449058 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:02.449307 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:02.449404 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:02.449493 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.449673 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.450366 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.450561 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.451254 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.452615 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.456737 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.457605 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.458022 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:02.458161 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:02.458289 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.458835 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:02.459141 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:02.459240 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.466396 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.466756 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.471615 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.471897 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:02.472176 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:02.481430 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.491677 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.492016 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.492892 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.493268 136134801375232 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0404 17:00:02.493526 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:02.493614 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:02.493686 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:02.493803 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.498045 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:02.517557 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.518497 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:02.522882 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:02.548546 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:02.548756 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:02.548860 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:02.548945 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.549135 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.549796 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.549994 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.550682 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.552007 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.556617 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.557460 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.557679 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:02.557760 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:02.557879 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.558360 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:02.558610 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:02.558701 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.568984 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.569519 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.576426 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.576722 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:02.576928 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:02.584502 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:02.591197 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.591524 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:02.592119 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:02.592364 136134801375232 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0404 17:00:02.597421 136134801375232 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:42.037216 136134801375232 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000\n",
            "/usr/local/lib/python3.10/dist-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.\n",
            "  warnings.warn(\n",
            "I0404 17:00:43.759450 136134801375232 training_loop.py:409] No working directory specified.\n",
            "I0404 17:00:43.759684 136134801375232 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:\n",
            "I0404 17:00:43.761291 136134801375232 checkpoints.py:429] Restoring checkpoint from ag_ckpt_vocab/checkpoint_10999999\n",
            "I0404 17:00:51.853335 136134801375232 training_loop.py:447] Only restoring trainable parameters.\n",
            "I0404 17:00:51.854590 136134801375232 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.854786 136134801375232 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.854898 136134801375232 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.855019 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.855132 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.855232 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.855328 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.855428 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.855520 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.855609 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.855697 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.855787 136134801375232 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.855877 136134801375232 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.855967 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.856054 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.856161 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.856247 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.856336 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.856433 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.856520 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.856608 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.856696 136134801375232 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.856788 136134801375232 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.856876 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.856971 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.857059 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.857163 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.857252 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.857350 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.857442 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.857531 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.857621 136134801375232 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.857711 136134801375232 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.857801 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.857890 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.857978 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.858065 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.858162 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.858249 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.858339 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.858436 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.858526 136134801375232 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.858617 136134801375232 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.858706 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.858795 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.858886 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.858975 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.859069 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.859171 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.859258 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.859356 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.859448 136134801375232 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.859539 136134801375232 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.859629 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.859719 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.859809 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.859898 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.859987 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.860075 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.860172 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.860259 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.860353 136134801375232 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.860442 136134801375232 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.860530 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.860618 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.860706 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.860794 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.860881 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.860969 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.861057 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.861155 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.861242 136134801375232 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.861331 136134801375232 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.861426 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.861514 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.861602 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.861689 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.861778 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.861868 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.861962 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.862050 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.862156 136134801375232 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.862246 136134801375232 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.862334 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.862430 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.862519 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.862606 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.862695 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.862782 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.862870 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.862961 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.863049 136134801375232 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.863153 136134801375232 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.863245 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.863333 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.863430 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.863522 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.863610 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.863697 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.863785 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.863874 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.863964 136134801375232 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.864053 136134801375232 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.864157 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.864247 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.864370 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.864459 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.864547 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.864637 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.864724 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.864812 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.864902 136134801375232 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.864992 136134801375232 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0404 17:00:51.865081 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0404 17:00:51.865180 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.865266 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.865361 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.865451 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.865539 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0404 17:00:51.865628 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0404 17:00:51.865716 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0404 17:00:51.915385 136134801375232 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0404 17:00:51.915648 136134801375232 training_loop.py:725] Total parameters: 152072288\n",
            "I0404 17:00:51.916535 136134801375232 training_loop.py:739] Total state size: 0\n",
            "I0404 17:00:52.260681 136134801375232 training_loop.py:492] Training loop: creating task for mode beam_search\n",
            "I0404 17:00:52.261082 136134801375232 training_loop.py:685] Creating logging writer (train) for mode beam_search\n",
            "I0404 17:00:52.261886 136134801375232 training_loop.py:652] Compiling mode beam_search with jit.\n",
            "I0404 17:00:52.262520 136134801375232 training_loop.py:89] registering functions: dict_keys([])\n",
            "I0404 17:00:52.272803 136134801375232 graph.py:498] orthocenter\n",
            "I0404 17:00:52.272997 136134801375232 graph.py:499] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b ? perp a d b c\n",
            "I0404 17:00:52.309872 136134801375232 ddar.py:60] Depth 1/1000 time = 0.004286527633666992\n",
            "I0404 17:00:52.315382 136134801375232 ddar.py:60] Depth 2/1000 time = 0.005261421203613281\n",
            "I0404 17:00:52.316026 136134801375232 alphageometry.py:221] DD+AR failed to solve the problem.\n",
            "I0404 17:00:52.316173 136134801375232 alphageometry.py:539] Depth 0. There are 1 nodes to expand:\n",
            "I0404 17:00:52.316245 136134801375232 alphageometry.py:543] {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00\n",
            "I0404 17:00:52.316294 136134801375232 alphageometry.py:548] Decoding from {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00\n",
            "I0404 17:00:52.418484 136134801375232 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.418827 136134801375232 decoder_stack.py:316] dstack: scanning over 1 windows.\n",
            "I0404 17:00:52.418976 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419057 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419158 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419238 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419297 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419365 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419426 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419482 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419538 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419594 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419649 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419703 136134801375232 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0404 17:00:52.419767 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.419823 136134801375232 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0404 17:00:52.420006 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:52.420076 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:52.420133 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:52.422712 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.427032 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:52.447070 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.447958 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:52.452195 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:52.460428 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:52.460628 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.460691 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:52.460746 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.460896 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.461538 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.461729 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.462344 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.463842 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.468021 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.468917 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.469179 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:52.469263 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:52.469362 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.469763 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:52.470310 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:52.470424 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.477797 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.478126 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.482586 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.482846 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:52.483479 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:52.490266 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.496193 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.496481 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.496966 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.497195 136134801375232 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0404 17:00:52.497376 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:52.497441 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:52.497487 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:52.499927 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.503982 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:52.523162 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.524563 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:52.533847 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:52.543196 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:52.543402 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.543467 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:52.543522 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.543660 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.544251 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.544471 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.545025 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.546473 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.550144 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.550803 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.550962 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:52.551017 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:52.551118 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.551464 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:52.551949 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:52.552042 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.558403 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.558653 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.562718 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.562930 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:52.563484 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:52.569882 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.575652 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.575925 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.576439 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.576648 136134801375232 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0404 17:00:52.576816 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:52.576879 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:52.576925 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:52.579358 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.583086 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:52.599610 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.600440 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:52.604528 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:52.612368 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:52.612550 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.612612 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:52.612667 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.612804 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.613446 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.613650 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.614225 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.615679 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.619510 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.620439 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.620673 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:52.620770 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:52.620912 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.621375 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:52.621880 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:52.621965 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.627857 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.628079 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.632493 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.632705 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:52.633302 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:52.640004 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.645665 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.645927 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.646431 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.646626 136134801375232 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0404 17:00:52.646788 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:52.646850 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:52.646896 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:52.649309 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.653066 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:52.670050 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.670891 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:52.675012 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:52.683120 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:52.683325 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.683390 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:52.683445 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.683581 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.684163 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.684398 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.684969 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.686570 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.690796 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.691765 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.692002 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:52.692131 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:52.692265 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.692884 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:52.693635 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:52.693730 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.701259 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.701564 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.706207 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.706472 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:52.707043 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:52.713600 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.719421 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.719700 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.720325 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.720628 136134801375232 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0404 17:00:52.720914 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:52.721033 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:52.721130 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:52.723886 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.727915 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:52.745490 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.746340 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:52.751450 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:52.759819 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:52.760019 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.760083 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:52.760185 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.760366 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.761008 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.761246 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.761885 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.763391 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.767289 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.768017 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.768227 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:52.768319 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:52.768444 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.768850 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:52.769402 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:52.769505 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.776070 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.776393 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.780787 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.781021 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:52.781618 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:52.788154 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.793948 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.794266 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.794831 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.795073 136134801375232 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0404 17:00:52.795309 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:52.795395 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:52.795469 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:52.798013 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.801973 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:52.819248 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.820129 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:52.826590 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:52.838798 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:52.838997 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.839090 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:52.839208 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.839391 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.840027 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.840247 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.840909 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.842636 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.846570 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.847291 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.847470 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:52.847551 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:52.847673 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.848067 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:52.848617 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:52.848717 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.854469 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.854706 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.858815 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.859044 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:52.859664 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:52.866057 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.871780 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.872076 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.872641 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.872863 136134801375232 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0404 17:00:52.873079 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:52.873206 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:52.873294 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:52.875794 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.879781 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:52.904526 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.905383 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:52.909468 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:52.917869 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:52.918174 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.918400 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:52.918538 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.918737 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.919453 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.919646 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.920341 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.922039 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.926753 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.927590 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.927821 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:52.927907 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:52.928000 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.928443 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:52.928987 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:52.929122 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.935230 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.935513 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.939810 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.940057 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:52.940646 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:52.947420 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:52.953128 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.953422 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:52.953913 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.954134 136134801375232 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0404 17:00:52.954317 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:52.954386 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:52.954431 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:52.956946 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.961031 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:52.978008 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.978892 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:52.983044 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:52.992142 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:52.992395 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:52.992508 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:52.992602 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.992801 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.993753 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.993998 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.994837 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:52.996610 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.000622 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.001470 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.001692 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.001773 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.001894 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.002340 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.002916 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.003022 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.009202 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.009502 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.014159 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.014463 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.015172 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.021857 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.031261 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.031649 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.032456 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.032757 136134801375232 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0404 17:00:53.033045 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.033170 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.033247 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.037107 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.043186 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.064912 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.066016 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.071542 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:53.087017 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.087283 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.087400 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.087495 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.087725 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.088634 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.088980 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.090004 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.092584 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.099089 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.099960 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.100210 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.100283 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.100374 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.100744 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.101336 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.101440 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.107943 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.108335 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.112792 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.113045 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.113691 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.120707 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.127251 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.127537 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.128071 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.128395 136134801375232 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0404 17:00:53.128628 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.128716 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.128799 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.131369 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.135422 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.152826 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.153811 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.158167 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:53.166612 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.166833 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.166936 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.167018 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.167207 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.167882 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.168081 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.168744 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.170305 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.174115 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.174869 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.175081 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.175176 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.175300 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.175711 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.176276 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.176391 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.182332 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.182622 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.186858 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.187132 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.187742 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.194772 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.200580 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.200896 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.201458 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.201692 136134801375232 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0404 17:00:53.201908 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.201995 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.202065 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.204575 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.208493 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.225780 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.226701 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.230758 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:53.239788 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.240001 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.240111 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.240195 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.240368 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.241025 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.241235 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.241887 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.243419 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.247314 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.247995 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.248180 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.248265 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.248387 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.248781 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.249334 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.249432 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.255157 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.255397 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.259476 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.259683 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.260290 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.266861 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.272654 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.272964 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.273530 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.273773 136134801375232 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0404 17:00:53.273989 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.274078 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.274171 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.276900 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.281878 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.298376 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.299281 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.304056 136134801375232 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0404 17:00:53.315608 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.315815 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.315907 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.315988 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.316191 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.316862 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.317061 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.317756 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.319385 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.323272 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.323978 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.324169 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.324243 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.324385 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.324896 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.326081 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.326220 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.337041 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.337440 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.343883 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.344212 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.345034 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.352989 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.358864 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.359188 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.359751 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.360297 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.360434 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.360538 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.360622 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.360704 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.360789 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.360875 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.360960 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.361043 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.361141 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.361229 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.361313 136134801375232 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0404 17:00:53.361390 136134801375232 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0404 17:00:53.366545 136134801375232 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0404 17:00:53.483626 136134801375232 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.483911 136134801375232 decoder_stack.py:333] dstack: autoregressive generator.\n",
            "I0404 17:00:53.484157 136134801375232 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0404 17:00:53.484582 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.484698 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.484775 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.484908 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.489179 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.506207 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.507124 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.511373 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:53.537235 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.537483 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.537597 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.537701 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.537940 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.538822 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.539060 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.539937 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.541827 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.548230 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.549127 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.549359 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.549427 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.549541 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.549967 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.550269 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.550371 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.560058 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.560430 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.564608 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.564852 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.565063 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.575250 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.584871 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.585269 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.586063 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.586387 136134801375232 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0404 17:00:53.586685 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.586805 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.586877 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.587013 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.593479 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.611465 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.612360 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.616486 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:53.637655 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.637850 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.637941 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.638020 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.638221 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.638856 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.639037 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.639682 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.640971 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.644870 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.645588 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.645773 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.645865 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.645983 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.646408 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.646643 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.646730 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.652849 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.653147 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.657217 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.657455 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.657664 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.663921 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.669711 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.670011 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.670572 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.670804 136134801375232 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0404 17:00:53.671011 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.671109 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.671184 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.671303 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.675488 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.692662 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.693541 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.697596 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:53.719079 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.719302 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.719393 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.719473 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.719643 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.720323 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.720513 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.721169 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.722484 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.726579 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.727324 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.727513 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.727589 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.727703 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.728091 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.728368 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.728459 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.734272 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.734549 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.739637 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.739892 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.740138 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.746766 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.753906 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.754240 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.754805 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.755033 136134801375232 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0404 17:00:53.755268 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.755370 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.755441 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.755563 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.759932 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.777215 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.778073 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.782361 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:53.803451 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.803654 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.803751 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.803831 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.803999 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.804679 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.804878 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.805573 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.806904 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.810690 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.811421 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.811609 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.811689 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.811806 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.812224 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.812461 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.812549 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.818971 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.819290 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.823620 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.823860 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.824073 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.830780 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.836555 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.836854 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.837634 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.837945 136134801375232 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0404 17:00:53.838229 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.838359 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.838440 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.838570 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.844729 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.864792 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.865666 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.869741 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:53.900943 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:53.901215 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:53.901340 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:53.901434 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.901614 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.902256 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.902453 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.903322 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.905058 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.911130 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.912207 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.912488 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:53.912580 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:53.912719 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.913300 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:53.913625 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:53.913749 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.923306 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.923639 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.929958 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.930295 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:53.930575 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:53.942011 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:53.951811 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.952299 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:53.953129 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.953478 136134801375232 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0404 17:00:53.953793 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:53.953930 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:53.954035 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:53.954213 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.960255 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:53.989675 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:53.991058 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:53.998357 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:54.035714 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:54.035945 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:54.036031 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:54.036128 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.036323 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.037126 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.037359 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.047970 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.050667 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.061928 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.063026 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.063352 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:54.063464 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:54.063603 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.064450 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:54.064776 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:54.064887 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.074383 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.078512 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.085877 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.087609 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:54.087973 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:54.097850 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.112089 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.113918 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.114902 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.115245 136134801375232 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0404 17:00:54.115540 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:54.115648 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:54.115725 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:54.115850 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.122165 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:54.154381 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.155840 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:54.162052 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:54.192799 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:54.193039 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:54.193140 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:54.193226 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.193428 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.194224 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.194497 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.195342 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.197137 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.202657 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.203634 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.203889 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:54.203968 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:54.204086 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.204644 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:54.204941 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:54.205041 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.214256 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.214627 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.220434 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.220764 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:54.221009 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:54.230414 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.238794 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.239173 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.239887 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.240155 136134801375232 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0404 17:00:54.240411 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:54.240513 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:54.240581 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:54.240690 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.246161 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:54.271907 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.273158 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:54.279482 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:54.311560 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:54.312150 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:54.312327 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:54.312433 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.312663 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.313832 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.314056 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.314973 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.316782 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.322438 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.323771 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.324036 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:54.324129 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:54.324252 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.324804 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:54.325114 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:54.325216 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.333779 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.334170 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.340322 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.340663 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:54.340932 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:54.355634 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.371412 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.374283 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.375369 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.375705 136134801375232 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0404 17:00:54.375983 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:54.376089 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:54.376177 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:54.376289 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.382203 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:54.417114 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.418366 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:54.424430 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:54.455599 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:54.455859 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:54.455954 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:54.456037 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.456255 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.457078 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.457359 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.458219 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.460239 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.466024 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.466991 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.467239 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:54.467319 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:54.467437 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.467931 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:54.468216 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:54.468314 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.476727 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.477108 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.482881 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.483218 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:54.483464 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:54.493045 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.501612 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.501995 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.502721 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.503000 136134801375232 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0404 17:00:54.503253 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:54.503352 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:54.503419 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:54.503529 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.508908 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:54.536679 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.537969 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:54.544497 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:54.576636 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:54.576884 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:54.576998 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:54.577110 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.577377 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.578309 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.578562 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.579540 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.581643 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.587786 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.588737 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.588980 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:54.589057 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:54.589192 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.589682 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:54.589983 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:54.590088 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.599124 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.599463 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.605051 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.605376 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:54.605611 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:54.614902 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.623384 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.623800 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.624548 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.624826 136134801375232 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0404 17:00:54.625064 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:54.625172 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:54.625240 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:54.625357 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.630660 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:54.660916 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.663109 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:54.670382 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:54.702366 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:54.702610 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:54.702697 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:54.702778 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.702967 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.703792 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.704067 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.704982 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.706837 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.712349 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.713342 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.713596 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:54.713671 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:54.713800 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.714303 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:54.714615 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:54.714713 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.723892 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.724311 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.730196 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.730506 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:54.730738 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:54.740182 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.748819 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.749512 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.750602 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.750950 136134801375232 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0404 17:00:54.751271 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:54.751394 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:54.751481 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:54.751596 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.757545 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:54.784497 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.785692 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:54.791535 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:54.821659 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:54.821897 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:54.821984 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:54.822064 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.822268 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.823052 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.823302 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.824139 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.825956 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.831385 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.832330 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.832586 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:54.832662 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:54.832778 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.833266 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:54.833550 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:54.833646 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.841938 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.842299 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.848232 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.848526 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:54.848754 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:54.857707 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:54.865780 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.866156 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:54.866863 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:54.867161 136134801375232 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0404 17:00:54.873766 136134801375232 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0404 17:00:55.091422 136134801375232 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.091831 136134801375232 decoder_stack.py:333] dstack: autoregressive generator.\n",
            "I0404 17:00:55.091979 136134801375232 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0404 17:00:55.092246 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:55.092363 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:55.092438 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:55.092569 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.100733 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:55.130722 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.132066 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:55.137991 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:55.178226 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:55.178451 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:55.178540 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:55.178623 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.178816 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.179665 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.179883 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.180822 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.183124 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.191888 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.193014 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.193375 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:55.193467 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:55.193595 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.194220 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:55.194546 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:55.194663 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.204978 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.205399 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.212269 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.212618 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:55.212922 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:55.227180 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.237262 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.237563 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.238170 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.238453 136134801375232 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0404 17:00:55.238722 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:55.238835 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:55.238911 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:55.239032 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.245130 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:55.276198 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.277431 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:55.283536 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:55.319852 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:55.321032 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:55.321250 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:55.321362 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.321603 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.322539 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.322798 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.323787 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.325669 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.331557 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.332531 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.332780 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:55.332854 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:55.332973 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.333489 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:55.333787 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:55.333907 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.343169 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.343539 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.349286 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.349612 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:55.349878 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:55.365073 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.375812 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.376818 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.377647 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.377985 136134801375232 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0404 17:00:55.378306 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:55.378443 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:55.378543 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:55.378668 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.384419 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:55.416851 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.418401 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:55.425505 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:55.463302 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:55.463548 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:55.463654 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:55.463742 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.463980 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.464925 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.465176 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.466128 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.467910 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.473403 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.474372 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.474619 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:55.474698 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:55.474830 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.475474 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:55.475834 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:55.475944 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.484860 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.485312 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.491203 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.491516 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:55.491765 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:55.502953 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.516721 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.517105 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.517940 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.518247 136134801375232 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0404 17:00:55.518528 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:55.518648 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:55.518735 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:55.518862 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.524734 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:55.554383 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.555566 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:55.561651 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:55.591752 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:55.592018 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:55.592124 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:55.592208 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.592404 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.593208 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.593460 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.594296 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.596002 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.601437 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.602358 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.602577 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:55.602654 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:55.602774 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.603259 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:55.603540 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:55.603637 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.612082 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.612452 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.618036 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.618357 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:55.618593 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:55.627861 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.636150 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.636520 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.637254 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.637528 136134801375232 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0404 17:00:55.637771 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:55.637867 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:55.637938 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:55.638057 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.644577 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:55.678979 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.680184 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:55.686372 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:55.716742 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:55.716983 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:55.717090 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:55.717192 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.717386 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.718199 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.718448 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.719321 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.721123 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.726647 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.727602 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.727846 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:55.727924 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:55.728052 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.728554 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:55.728831 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:55.728926 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.737788 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.738176 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.744345 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.744674 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:55.744923 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:55.756795 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.766254 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.766615 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.767390 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.767660 136134801375232 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0404 17:00:55.767901 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:55.767999 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:55.768070 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:55.768194 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.773676 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:55.798448 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.799635 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:55.805439 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:55.835703 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:55.835930 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:55.836028 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:55.836123 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.836328 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.837133 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.837376 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.838217 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.839961 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.845623 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.846539 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.846767 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:55.846844 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:55.846963 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.847535 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:55.847814 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:55.847962 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.857340 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.857683 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.863575 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.863877 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:55.864173 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:55.874620 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.883028 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.883397 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.884091 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.884396 136134801375232 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0404 17:00:55.884643 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:55.884734 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:55.884804 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:55.884912 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.890271 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:55.914426 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.915632 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:55.921368 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:55.951037 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:55.951288 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:55.951383 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:55.951470 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.951665 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.952466 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.952701 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.955528 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.961174 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.966853 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.967868 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.968366 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:55.968714 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:55.968852 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.969424 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:55.969691 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:55.969785 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:55.978627 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.978995 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:55.984873 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:55.985241 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:55.985502 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:55.995156 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.003540 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.003896 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.004641 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.004911 136134801375232 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0404 17:00:56.005169 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:56.005265 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:56.005332 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:56.005441 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.010736 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:56.035723 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.036891 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:56.042706 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:56.077496 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:56.077735 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:56.077864 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:56.077965 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.078267 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.079192 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.079450 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.080467 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.082523 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.088518 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.089339 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.089534 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:56.089607 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:56.089721 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.090154 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:56.090386 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:56.090471 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.099239 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.099623 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.106111 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.106441 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:56.106686 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:56.116121 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.124492 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.124851 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.125560 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.125830 136134801375232 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0404 17:00:56.126066 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:56.126171 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:56.126240 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:56.126347 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.131600 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:56.156886 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.158072 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:56.164645 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:56.194516 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:56.194755 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:56.194843 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:56.194923 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.195141 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.195959 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.196207 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.197053 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.198795 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.204170 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.205128 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.205371 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:56.205463 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:56.205600 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.208585 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:56.208928 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:56.209037 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.217584 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.217924 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.229038 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.229414 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:56.229704 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:56.240653 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.253660 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.254078 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.257135 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.257500 136134801375232 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0404 17:00:56.257793 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:56.257889 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:56.257965 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:56.258076 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.269687 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:56.308775 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.309979 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:56.315866 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:56.349939 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:56.350193 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:56.350284 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:56.350365 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.350555 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.351347 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.351581 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.352406 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.354121 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.360625 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.361615 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.361846 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:56.361923 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:56.362052 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.362556 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:56.362931 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:56.363056 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.375312 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.375688 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.384069 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.384462 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:56.384728 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:56.394182 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.403075 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.403446 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.404152 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.404430 136134801375232 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0404 17:00:56.404657 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:56.404745 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:56.404813 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:56.404926 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.410265 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:56.435461 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.437021 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:56.446328 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:56.477312 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:56.477558 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:56.477649 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:56.477728 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.477931 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.478727 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.478992 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.479849 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.481641 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.487167 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.488124 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.488358 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:56.488436 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:56.488565 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.489070 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:56.489358 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:56.489454 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.497683 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.498019 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.503648 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.503973 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:56.504231 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:56.513306 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.521916 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.522281 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.522973 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.523278 136134801375232 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0404 17:00:56.523526 136134801375232 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0404 17:00:56.523618 136134801375232 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0404 17:00:56.523685 136134801375232 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0404 17:00:56.523791 136134801375232 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.529361 136134801375232 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0404 17:00:56.555913 136134801375232 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.560905 136134801375232 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0404 17:00:56.572703 136134801375232 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0404 17:00:56.611637 136134801375232 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0404 17:00:56.611919 136134801375232 attention.py:418] Single window, no scan.\n",
            "I0404 17:00:56.612035 136134801375232 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0404 17:00:56.612155 136134801375232 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.612394 136134801375232 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.613301 136134801375232 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.613516 136134801375232 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.614451 136134801375232 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.616405 136134801375232 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.625003 136134801375232 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.626092 136134801375232 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.626409 136134801375232 transformer_layer.py:468] tlayer: End windows.\n",
            "I0404 17:00:56.626513 136134801375232 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0404 17:00:56.626652 136134801375232 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.627246 136134801375232 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0404 17:00:56.627582 136134801375232 nn_components.py:325] mlp: activation = None\n",
            "I0404 17:00:56.627704 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.637336 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.637719 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.644655 136134801375232 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.644989 136134801375232 transformer_base.py:443] tbase: final FFN\n",
            "I0404 17:00:56.645281 136134801375232 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0404 17:00:56.659075 136134801375232 nn_components.py:329] mlp: final activation = None\n",
            "I0404 17:00:56.667395 136134801375232 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.667696 136134801375232 nn_components.py:261] mlp: residual\n",
            "I0404 17:00:56.668442 136134801375232 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:00:56.668775 136134801375232 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0404 17:00:56.675815 136134801375232 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0404 17:02:42.033774 136134801375232 alphageometry.py:565] LM output (score=-1.123075): \"e : C a c e 02 C b d e 03 ;\"\n",
            "I0404 17:02:42.034059 136134801375232 alphageometry.py:566] Translation: \"e = on_line e a c, on_line e b d\"\n",
            "\n",
            "I0404 17:02:42.034182 136134801375232 alphageometry.py:575] Solving: \"a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c\"\n",
            "I0404 17:02:42.034387 136134801375232 graph.py:498] \n",
            "I0404 17:02:42.034483 136134801375232 graph.py:499] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c\n",
            "I0404 17:02:42.064670 136134801375232 ddar.py:60] Depth 1/1000 time = 0.024628162384033203\n",
            "I0404 17:02:42.093674 136134801375232 ddar.py:60] Depth 2/1000 time = 0.028748512268066406\n",
            "I0404 17:02:42.141731 136134801375232 ddar.py:60] Depth 3/1000 time = 0.04780006408691406\n",
            "I0404 17:02:42.145624 136134801375232 alphageometry.py:191] \n",
            "==========================\n",
            " * From theorem premises:\n",
            "A B C D : Points\n",
            "BD  AC [00]\n",
            "CD  AB [01]\n",
            "\n",
            " * Auxiliary Constructions:\n",
            "E : Points\n",
            "B,D,E are collinear [02]\n",
            "A,C,E are collinear [03]\n",
            "\n",
            " * Proof steps:\n",
            "001. B,D,E are collinear [02] & A,C,E are collinear [03] & BD  AC [00]   BEA = CED [04]\n",
            "002. B,D,E are collinear [02] & A,C,E are collinear [03] & BD  AC [00]   BEC = AED [05]\n",
            "003. BD  AC [00] & CD  AB [01]   CDB = BAC [06]\n",
            "004. A,C,E are collinear [03] & B,D,E are collinear [02] & BAC = CDB [06]   BAE = CDE [07]\n",
            "005. BEA = CED [04] & BAE = CDE [07] (Similar Triangles)  BE:CE = AE:DE [08]\n",
            "006. BE:CE = AE:DE [08] & BEC = AED [05] (Similar Triangles)  BCE = ADE [09]\n",
            "007. BE:CE = AE:DE [08] & BEC = AED [05] (Similar Triangles)  EBC = EAD [10]\n",
            "008. BCE = ADE [09] & A,C,E are collinear [03] & B,D,E are collinear [02] & EBC = EAD [10]   AD  BC\n",
            "==========================\n",
            "\n",
            "I0404 17:02:42.191241 136134801375232 alphageometry.py:581] Solved.\n"
          ]
        }
      ]
    }
  ]
}