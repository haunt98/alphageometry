{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPaWhvqW8NWI",
        "outputId": "f8a880d7-ca92-478a-b1ac-f3f6fd4e813b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd /content\n",
        "!rm -rf /content/alphageometry\n",
        "!git clone --depth 1 https://github.com/haunt98/alphageometry.git\n",
        "%cd /content/alphageometry"
      ],
      "metadata": {
        "id": "BGyLJ6HO8PWR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.in"
      ],
      "metadata": {
        "id": "BKVao0qG8VIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eccf4d5-f960-4943-db4d-54e44ca06514"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 1)) (2.14.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 2)) (1.23.5)\n",
            "Requirement already satisfied: scipy==1.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 3)) (1.10.0)\n",
            "Requirement already satisfied: matplotlib==3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 4)) (3.7.0)\n",
            "Requirement already satisfied: gdown==4.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 5)) (4.7.1)\n",
            "Requirement already satisfied: jax==0.4.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 6)) (0.4.6)\n",
            "Requirement already satisfied: jaxlib==0.4.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 7)) (0.4.6)\n",
            "Requirement already satisfied: flax==0.5.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 8)) (0.5.3)\n",
            "Requirement already satisfied: gin-config==0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 9)) (0.5.0)\n",
            "Requirement already satisfied: gin==0.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 10)) (0.1.6)\n",
            "Requirement already satisfied: t5==0.9.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 11)) (0.9.4)\n",
            "Requirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 12)) (0.1.99)\n",
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 13)) (1.4.0)\n",
            "Requirement already satisfied: clu==0.0.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 14)) (0.0.7)\n",
            "Requirement already satisfied: optax==0.1.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 15)) (0.1.7)\n",
            "Requirement already satisfied: seqio==0.0.18 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 16)) (0.0.18)\n",
            "Requirement already satisfied: tensorflow-datasets==4.9.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 17)) (4.9.3)\n",
            "Requirement already satisfied: tensorflow-text==2.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.in (line 18)) (2.14.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (4.13.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0->-r requirements.in (line 1)) (2.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.in (line 4)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.in (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.in (line 4)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.in (line 4)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.in (line 4)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.in (line 4)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.in (line 4)) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==4.7.1->-r requirements.in (line 5)) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown==4.7.1->-r requirements.in (line 5)) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown==4.7.1->-r requirements.in (line 5)) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown==4.7.1->-r requirements.in (line 5)) (4.13.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax==0.5.3->-r requirements.in (line 8)) (1.1.0)\n",
            "Requirement already satisfied: rich~=11.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.5.3->-r requirements.in (line 8)) (11.2.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax==0.5.3->-r requirements.in (line 8)) (0.1.45)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.5.3->-r requirements.in (line 8)) (6.0.2)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (2.17.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (0.8.1)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (4.2.1)\n",
            "Requirement already satisfied: mesh-tensorflow>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.9.4->-r requirements.in (line 11)) (0.1.21)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (2.2.2)\n",
            "Requirement already satisfied: rouge-score>=0.1.2 in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (1.6.1)\n",
            "Requirement already satisfied: seqio-nightly in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (0.0.18.dev20240524)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (4.9.2.dev202308090034)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from t5==0.9.4->-r requirements.in (line 11)) (4.50.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.11/dist-packages (from clu==0.0.7->-r requirements.in (line 14)) (2.0.1)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from clu==0.0.7->-r requirements.in (line 14)) (1.12.2)\n",
            "Requirement already satisfied: ml-collections in /usr/local/lib/python3.11/dist-packages (from clu==0.0.7->-r requirements.in (line 14)) (1.0.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from optax==0.1.7->-r requirements.in (line 15)) (0.1.7)\n",
            "Requirement already satisfied: pyglove in /usr/local/lib/python3.11/dist-packages (from seqio==0.0.18->-r requirements.in (line 16)) (0.4.4)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (0.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (8.1.8)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (0.1.9)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (5.9.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (0.10.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text==2.14.0->-r requirements.in (line 18)) (0.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0->-r requirements.in (line 1)) (0.45.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.5->optax==0.1.7->-r requirements.in (line 15)) (0.12.1)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (25.3.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (3.21.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from mesh-tensorflow>=0.1.13->mesh-tensorflow[transformer]>=0.1.13->t5==0.9.4->-r requirements.in (line 11)) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.7.1->-r requirements.in (line 5)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.7.1->-r requirements.in (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.7.1->-r requirements.in (line 5)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.7.1->-r requirements.in (line 5)) (2025.1.31)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from rich~=11.1->flax==0.5.3->-r requirements.in (line 8)) (0.4.6)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from rich~=11.1->flax==0.5.3->-r requirements.in (line 8)) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from rich~=11.1->flax==0.5.3->-r requirements.in (line 8)) (2.18.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (3.1.3)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text==2.14.0->-r requirements.in (line 18)) (2.15.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.7.0->t5==0.9.4->-r requirements.in (line 11)) (0.30.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.7.0->t5==0.9.4->-r requirements.in (line 11)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.7.0->t5==0.9.4->-r requirements.in (line 11)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.7.0->t5==0.9.4->-r requirements.in (line 11)) (0.5.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==4.7.1->-r requirements.in (line 5)) (2.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->t5==0.9.4->-r requirements.in (line 11)) (1.4.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->t5==0.9.4->-r requirements.in (line 11)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->t5==0.9.4->-r requirements.in (line 11)) (2025.2)\n",
            "Requirement already satisfied: docstring-parser>=0.12 in /usr/local/lib/python3.11/dist-packages (from pyglove->seqio==0.0.18->-r requirements.in (line 16)) (0.16)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.7.1->-r requirements.in (line 5)) (1.7.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu->t5==0.9.4->-r requirements.in (line 11)) (3.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu->t5==0.9.4->-r requirements.in (line 11)) (0.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu->t5==0.9.4->-r requirements.in (line 11)) (5.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->t5==0.9.4->-r requirements.in (line 11)) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets==4.9.3->-r requirements.in (line 17)) (1.69.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0->-r requirements.in (line 1)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache models from https://bit.ly/alphageometry\n",
        "!mkdir /content/alphageometry/ag_ckpt_vocab\n",
        "!cp /content/gdrive/MyDrive/cache/* /content/alphageometry/ag_ckpt_vocab/\n",
        "!ls /content/alphageometry/ag_ckpt_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_vQj9rW_LJ0",
        "outputId": "711fd8d4-a5c6-4340-ffd9-018ab5ca1db2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint_10999999  geometry.757.model  geometry.757.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%env MELIAD_PATH=meliad_lib/meliad\n",
        "!mkdir -p $MELIAD_PATH\n",
        "!git clone --depth 1 https://github.com/google-research/meliad $MELIAD_PATH"
      ],
      "metadata": {
        "id": "eVsjbt_vCacX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/a/57240319\n",
        "import os\n",
        "\n",
        "print('Before PYTHONPATH', os.environ['PYTHONPATH'])\n",
        "# os.environ['PYTHONPATH'] = '/env/python'\n",
        "\n",
        "if '/content/alphageometry/meliad_lib/meliad' not in os.environ['PYTHONPATH']:\n",
        "  os.environ['PYTHONPATH'] += ':/content/alphageometry/meliad_lib/meliad'\n",
        "\n",
        "print('After PYTHONPATH', os.environ['PYTHONPATH'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN7k3Tp0Ckr3",
        "outputId": "90832f72-0d4f-4759-c297-1a9b0c259ae3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before PYTHONPATH /env/python\n",
            "After PYTHONPATH /env/python:/content/alphageometry/meliad_lib/meliad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m alphageometry \\\n",
        "  --alsologtostderr \\\n",
        "  --problems_file=examples.txt \\\n",
        "  --problem_name=orthocenter \\\n",
        "  --mode=alphageometry \\\n",
        "  --defs_file=defs.txt \\\n",
        "  --rules_file=rules.txt \\\n",
        "  --beam_size=2 \\\n",
        "  --search_depth=2 \\\n",
        "  --ckpt_path=ag_ckpt_vocab \\\n",
        "  --vocab_path=ag_ckpt_vocab/geometry.757.model \\\n",
        "  --gin_search_paths=$MELIAD_PATH/transformer/configs \\\n",
        "  --gin_file=base_htrans.gin \\\n",
        "  --gin_file=size/medium_150M.gin \\\n",
        "  --gin_file=options/positions_t5.gin \\\n",
        "  --gin_file=options/lr_cosine_decay.gin \\\n",
        "  --gin_file=options/seq_1024_nocache.gin \\\n",
        "  --gin_file=geometry_150M_generate.gin \\\n",
        "  --gin_param=DecoderOnlyLanguageModelGenerate.output_token_losses=True \\\n",
        "  --gin_param=TransformerTaskConfig.batch_size=2 \\\n",
        "  --gin_param=TransformerTaskConfig.sequence_length=128 \\\n",
        "  --gin_param=Trainer.restore_state_variables=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvVexyo7Dvbp",
        "outputId": "7b127dd3-965e-4444-8fb8-02e629f1cb99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.11/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.11/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "2025-04-12 04:07:12.635374: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-04-12 04:07:12.635493: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-04-12 04:07:12.635589: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-12 04:07:14.669624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.11/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.11/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.11/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "I0412 04:07:21.450866 133632064819200 inference_utils.py:69] Parsing gin configuration.\n",
            "I0412 04:07:21.451052 133632064819200 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs\n",
            "I0412 04:07:21.451498 133632064819200 inference_utils.py:74] Loading Gin config file base_htrans.gin\n",
            "I0412 04:07:21.451571 133632064819200 inference_utils.py:74] Loading Gin config file size/medium_150M.gin\n",
            "I0412 04:07:21.451647 133632064819200 inference_utils.py:74] Loading Gin config file options/positions_t5.gin\n",
            "I0412 04:07:21.451717 133632064819200 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin\n",
            "I0412 04:07:21.451781 133632064819200 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin\n",
            "I0412 04:07:21.451865 133632064819200 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin\n",
            "I0412 04:07:21.451956 133632064819200 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True\n",
            "I0412 04:07:21.452016 133632064819200 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=2\n",
            "I0412 04:07:21.452069 133632064819200 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128\n",
            "I0412 04:07:21.452120 133632064819200 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False\n",
            "I0412 04:07:21.452239 133632064819200 resource_reader.py:50] system_path_file_exists:base_htrans.gin\n",
            "E0412 04:07:21.452494 133632064819200 resource_reader.py:55] Path not found: base_htrans.gin\n",
            "I0412 04:07:21.452838 133632064819200 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin\n",
            "E0412 04:07:21.453033 133632064819200 resource_reader.py:55] Path not found: trainer_configuration.gin\n",
            "I0412 04:07:21.463163 133632064819200 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin\n",
            "E0412 04:07:21.463555 133632064819200 resource_reader.py:55] Path not found: size/medium_150M.gin\n",
            "I0412 04:07:21.464121 133632064819200 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin\n",
            "E0412 04:07:21.464399 133632064819200 resource_reader.py:55] Path not found: options/positions_t5.gin\n",
            "I0412 04:07:21.464968 133632064819200 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin\n",
            "E0412 04:07:21.465187 133632064819200 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin\n",
            "I0412 04:07:21.465948 133632064819200 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin\n",
            "E0412 04:07:21.466192 133632064819200 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin\n",
            "I0412 04:07:21.481157 133632064819200 training_loop.py:334] ==== Training loop: initializing model ====\n",
            "I0412 04:07:21.491039 133632064819200 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.\n",
            "I0412 04:07:21.491309 133632064819200 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'\n",
            "I0412 04:07:21.491466 133632064819200 xla_bridge.py:413] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
            "I0412 04:07:21.491554 133632064819200 xla_bridge.py:413] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
            "I0412 04:07:21.503514 133632064819200 xla_bridge.py:413] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.\n",
            "I0412 04:07:21.503927 133632064819200 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
            "W0412 04:07:21.504069 133632064819200 xla_bridge.py:420] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "I0412 04:07:21.504182 133632064819200 training_loop.py:335] Process 0 of 1\n",
            "I0412 04:07:21.504249 133632064819200 training_loop.py:336] Local device count = 1\n",
            "I0412 04:07:21.504376 133632064819200 training_loop.py:337] Number of replicas = 1\n",
            "I0412 04:07:21.504432 133632064819200 training_loop.py:339] Using random number seed 42\n",
            "I0412 04:07:22.426405 133632064819200 training_loop.py:359] Initializing the model.\n",
            "I0412 04:07:22.677459 133632064819200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.678202 133632064819200 decoder_stack.py:316] dstack: scanning over 1 windows.\n",
            "I0412 04:07:22.678384 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.678473 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.678549 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.678620 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.678689 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.678760 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.678832 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.678912 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.678983 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.679060 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.679133 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.679202 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.\n",
            "I0412 04:07:22.679259 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:22.679351 133632064819200 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0412 04:07:22.679550 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:22.679617 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:22.679668 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:22.682730 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.699932 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:22.754373 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.755388 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:22.770629 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:22.807347 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:22.807526 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:22.807599 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:22.807668 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.807840 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.810427 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.810672 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.812365 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.819156 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.834666 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.837801 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.838070 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:22.838157 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:22.838258 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.838670 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:22.839260 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:22.839383 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:22.853166 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.853464 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:22.861127 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:22.861397 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:22.862014 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:22.910550 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.347068 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.347403 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.348146 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.348404 133632064819200 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0412 04:07:23.348641 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:23.348731 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:23.348798 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:23.352246 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.357970 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:23.392740 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.393762 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:23.398763 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:23.408093 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:23.408259 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:23.408356 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:23.408426 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.408575 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.409253 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.409441 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.410093 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.411762 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.416213 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.417113 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.417318 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:23.417392 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:23.417494 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.417896 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:23.418441 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:23.418532 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.432695 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.432988 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.438181 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.438431 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:23.439093 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:23.457361 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.474846 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.478996 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.479894 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.480185 133632064819200 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0412 04:07:23.480455 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:23.480545 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:23.480610 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:23.484344 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.493061 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:23.526726 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.527699 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:23.533478 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:23.543440 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:23.543632 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:23.543709 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:23.543777 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.543933 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.544633 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.544935 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.545727 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.547649 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.552508 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.553332 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.553527 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:23.553608 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:23.553704 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.554108 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:23.554666 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:23.554766 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.569008 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.569332 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.575523 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.575857 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:23.576613 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:23.593707 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.609149 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.609583 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.610304 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.610550 133632064819200 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0412 04:07:23.610768 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:23.610866 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:23.610924 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:23.614212 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.619240 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:23.656832 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.657840 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:23.663122 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:23.672645 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:23.672817 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:23.672900 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:23.672978 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.673143 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.673823 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.674013 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.674724 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.676490 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.681126 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.681962 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.682191 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:23.682267 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:23.682389 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.682783 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:23.683370 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:23.683468 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.697844 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.698142 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.704826 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.705136 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:23.705892 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:23.721676 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.736713 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.736995 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.737632 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.737854 133632064819200 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0412 04:07:23.738058 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:23.738137 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:23.738191 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:23.741429 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.746614 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:23.776013 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.777055 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:23.781878 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:23.793713 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:23.793892 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:23.793967 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:23.794036 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.794199 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.794862 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.795042 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.795720 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.797455 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.802581 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.803370 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.803563 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:23.803633 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:23.803730 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.804124 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:23.804680 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:23.804768 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.818798 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.819078 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.824229 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.824508 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:23.825156 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:23.845321 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.860527 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.860800 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.861483 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.861704 133632064819200 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0412 04:07:23.861912 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:23.861983 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:23.862048 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:23.865113 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.870188 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:23.903249 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.904230 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:23.909140 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:23.919407 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:23.919596 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:23.919674 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:23.919744 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.919890 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.920564 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.920750 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.921427 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.923150 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.927759 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.928518 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.928694 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:23.928762 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:23.928858 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.929244 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:23.929777 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:23.929862 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.944121 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.944430 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.949572 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.949815 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:23.950467 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:23.965412 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:23.981074 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.981393 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:23.981978 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.982191 133632064819200 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0412 04:07:23.982410 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:23.982485 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:23.982538 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:23.985585 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:23.990609 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:24.027399 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.028662 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:24.034050 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:24.045932 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:24.046115 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:24.046193 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:24.046261 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.046441 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.047102 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.047311 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.047992 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.049792 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.055128 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.056031 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.056251 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:24.056362 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:24.056466 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.056918 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:24.057514 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:24.057621 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.072548 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.075369 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.081851 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.082130 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:24.082807 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:24.098182 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.112172 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.112461 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.113034 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.113224 133632064819200 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0412 04:07:24.113457 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:24.113527 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:24.113582 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:24.116671 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.121573 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:24.153039 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.154045 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:24.158989 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:24.168151 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:24.168330 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:24.168412 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:24.168484 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.168628 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.169249 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.169435 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.170061 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.171691 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.176130 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.176883 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.177057 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:24.177126 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:24.177226 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.177611 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:24.178131 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:24.178216 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.192333 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.192601 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.197754 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.198007 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:24.198618 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:24.213757 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.228177 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.228513 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.229103 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.229311 133632064819200 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0412 04:07:24.229522 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:24.229589 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:24.229640 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:24.232565 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.237611 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:24.274163 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.275153 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:24.280031 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:24.289488 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:24.289668 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:24.289743 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:24.289811 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.289972 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.290631 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.290807 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.291490 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.293159 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.297716 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.298708 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.298937 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:24.299010 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:24.299113 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.299545 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:24.300193 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:24.300318 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.314673 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.314939 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.319929 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.320158 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:24.320783 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:24.336991 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.351160 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.351457 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.352054 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.352261 133632064819200 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0412 04:07:24.352521 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:24.352588 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:24.352641 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:24.355612 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.361085 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:24.389944 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.391000 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:24.396691 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:24.408435 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:24.408608 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:24.408689 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:24.408755 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.408916 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.409569 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.409749 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.410411 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.412080 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.417117 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.417897 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.418095 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:24.418164 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:24.418264 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.418676 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:24.419239 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:24.419357 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.433523 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.433801 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.439054 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.439320 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:24.439991 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:24.454792 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.469246 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.469545 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.470108 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.470334 133632064819200 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0412 04:07:24.470526 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:24.470592 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:24.470644 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:24.473593 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.478565 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:24.513977 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.515175 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:24.521094 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:24.531109 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:24.531296 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:24.531373 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:24.531439 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.531581 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.532343 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.532556 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.533287 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.535111 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.541401 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.542197 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.542407 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:24.542479 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:24.542575 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.542955 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:24.543516 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:24.543617 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.559179 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.559495 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.564980 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.565439 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:24.566143 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:24.581158 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.594756 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.595051 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.595659 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.595962 133632064819200 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0412 04:07:24.596362 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:24.596444 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:24.596497 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:24.599804 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.604954 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:24.635067 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.636070 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:24.641119 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:07:24.650307 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:24.650491 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:24.650562 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:24.650630 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.650787 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.651481 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.651664 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.652336 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.654033 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.658706 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.659465 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.659663 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:24.659733 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:24.659876 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.660337 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:24.660903 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:24.660999 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.676924 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.677240 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.683143 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.685392 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:24.686438 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:24.704362 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:24.719897 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.720209 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:24.720945 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.721613 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723069 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723194 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723297 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723384 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723457 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723526 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723598 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723667 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723731 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723806 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723891 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.\n",
            "I0412 04:07:24.723968 133632064819200 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0412 04:07:24.732133 133632064819200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:07:24.973640 133632064819200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:24.973946 133632064819200 decoder_stack.py:333] dstack: autoregressive generator.\n",
            "I0412 04:07:24.974069 133632064819200 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0412 04:07:24.974401 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:24.974555 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:24.974671 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:24.974866 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:24.979916 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.000347 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.001297 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.006305 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.047722 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.048016 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.048222 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.048497 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.048729 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.051139 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.052629 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.054825 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.061084 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.073980 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.076968 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.077309 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.077483 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.077680 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.078477 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.078810 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.078912 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.087154 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.088803 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.094609 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.094932 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.095245 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.104341 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.112464 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.112765 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.113472 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.114837 133632064819200 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0412 04:07:25.115189 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.115365 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.115505 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.115703 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.120671 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.146332 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.147587 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.153365 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.186343 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.186618 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.186833 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.187094 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.187361 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.188339 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.188550 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.189493 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.191574 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.197110 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.198094 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.198369 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.198502 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.198689 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.199478 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.199758 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.199851 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.210666 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.211027 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.216478 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.216734 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.216959 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.223918 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.230977 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.231292 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.231964 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.232233 133632064819200 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0412 04:07:25.232557 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.232695 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.232814 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.233006 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.237713 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.262760 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.263859 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.271206 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.306604 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.306884 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.307140 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.307421 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.307651 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.308617 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.308838 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.309931 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.312110 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.317253 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.318166 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.318425 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.318561 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.318739 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.319489 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.319772 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.319869 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.328037 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.328318 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.334043 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.334327 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.334606 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.343861 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.351517 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.351811 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.352525 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.352779 133632064819200 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0412 04:07:25.353102 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.353250 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.353402 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.353591 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.358619 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.381690 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.382776 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.388834 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.423942 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.424213 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.424450 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.424691 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.424914 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.425870 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.426090 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.427086 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.429473 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.435583 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.436781 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.437067 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.437216 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.437452 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.438227 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.438551 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.438646 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.446561 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.446834 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.459839 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.460166 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.460489 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.469156 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.476743 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.477038 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.477710 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.477983 133632064819200 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0412 04:07:25.478325 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.478476 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.478607 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.478795 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.483807 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.508097 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.509221 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.515138 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.548931 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.549206 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.549452 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.549675 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.549899 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.550850 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.551073 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.552096 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.554365 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.560630 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.561627 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.561852 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.561999 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.562213 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.562963 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.563259 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.563381 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.575254 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.575593 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.581453 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.581759 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.582051 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.591745 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.599541 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.599835 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.600549 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.600801 133632064819200 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0412 04:07:25.601153 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.601337 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.601476 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.601678 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.607427 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.629549 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.630260 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.634001 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.658602 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.658912 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.659154 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.659379 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.659593 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.660528 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.660719 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.661467 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.662957 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.667042 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.667815 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.668009 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.668135 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.668333 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.668882 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.669109 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.669182 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.674453 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.674639 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.678264 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.678504 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.678714 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.684359 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.689357 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.689561 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.690054 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.690252 133632064819200 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0412 04:07:25.690529 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.690651 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.690766 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.690928 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.694169 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.707664 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.708461 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.711920 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.732594 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.732818 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.732992 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.733171 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.733367 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.734060 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.734250 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.734975 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.736508 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.740440 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.741145 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.741357 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.741477 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.741638 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.742197 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.742438 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.742512 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.747753 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.748084 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.752512 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.752748 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.752984 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.759008 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.763877 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.764103 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.764647 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.764870 133632064819200 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0412 04:07:25.765134 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.765261 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.765408 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.765579 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.768803 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.782871 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.783618 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.787615 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.810797 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.811021 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.811198 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.811407 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.811586 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.812468 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.812662 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.813431 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.814982 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.818931 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.819674 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.819871 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.819990 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.820155 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.820718 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.820935 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.821009 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.826962 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.827188 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.830671 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.830887 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.831107 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.836613 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.841291 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.841518 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.841996 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.842208 133632064819200 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0412 04:07:25.842473 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.842590 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.842699 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.842864 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.846081 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.860364 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.861076 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.864732 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.886077 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.886327 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.886501 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.886687 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.886866 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.887856 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.888067 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.888867 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.890630 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.895050 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.895808 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.896013 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.896131 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.896315 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.896902 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.897127 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.897200 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.902470 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.902666 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.906158 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.906381 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.906594 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.912175 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.916993 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.917221 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.917719 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.917997 133632064819200 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0412 04:07:25.918260 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.918420 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.918529 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.918694 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.921918 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:25.935870 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.936604 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:25.940188 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:25.966802 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:25.967025 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:25.967180 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:25.967404 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.967580 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.968249 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.968461 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.969160 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.971105 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.975356 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.976118 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.976371 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:25.976518 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:25.976700 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.977308 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:25.977553 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:25.977635 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.982931 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.983149 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.987039 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.987355 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:25.987647 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:25.993304 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:25.998182 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.998429 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:25.998930 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:25.999141 133632064819200 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0412 04:07:25.999420 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:25.999541 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:25.999652 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:25.999831 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.003010 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:26.018171 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.018921 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:26.022465 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:26.043576 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:26.043791 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:26.043961 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:26.044152 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.044357 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.045048 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.045230 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.046010 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.047580 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.051573 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.052323 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.052513 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:26.052629 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:26.052785 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.053391 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:26.053612 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:26.053696 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.058930 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.059132 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.062736 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.062971 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:26.063184 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:26.068796 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.073719 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.073971 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.074512 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.074733 133632064819200 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0412 04:07:26.075003 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:26.075124 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:26.075239 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:26.075423 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.078756 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:26.092843 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.093575 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:26.097223 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:26.118204 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:26.118443 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:26.118617 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:26.118811 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.118997 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.119699 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.119883 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.120607 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.122093 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.125997 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.126749 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.126947 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:26.127076 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:26.127237 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.127846 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:26.128064 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:26.128139 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.133571 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.133804 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.137516 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.137764 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:26.137995 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:26.143709 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.148621 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.148861 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.149374 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.149600 133632064819200 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0412 04:07:26.156814 133632064819200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:07:26.275092 133632064819200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.275372 133632064819200 decoder_stack.py:333] dstack: autoregressive generator.\n",
            "I0412 04:07:26.275475 133632064819200 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0412 04:07:26.275676 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:26.275794 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:26.275864 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:26.275966 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.283795 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:26.303131 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.304977 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:26.313165 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:26.354488 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:26.354641 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:26.354717 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:26.354786 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.354983 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.355661 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.355863 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.356729 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.358192 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.367667 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.368599 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.368845 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:26.368922 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:26.369028 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.369477 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:26.369693 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:26.369804 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.375769 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.376060 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.382771 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.383032 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:26.383232 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:26.389806 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.395416 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.395708 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.396432 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.396658 133632064819200 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0412 04:07:26.396837 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:26.396903 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:26.396962 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:26.397061 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.403124 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:26.423625 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.425480 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:26.434189 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:26.475370 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:26.475526 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:26.475602 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:26.475674 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.475883 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.476532 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.476751 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.477586 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.478986 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.484576 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.485254 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.485505 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:26.485583 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:26.485679 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.486078 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:26.486296 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:26.486413 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.492391 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.492661 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.499156 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.499431 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:26.499626 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:26.506104 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.511642 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.511953 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.512639 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.512851 133632064819200 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0412 04:07:26.513034 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:26.513101 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:26.513160 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:26.513254 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.520577 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:26.539191 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.541141 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:26.549697 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:26.596444 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:26.596634 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:26.596716 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:26.596785 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.597031 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.597735 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.597938 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.598871 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.600387 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.606028 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.606751 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.606994 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:26.607084 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:26.607183 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.607591 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:26.607786 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:26.607891 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.613779 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.614067 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.620873 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.621133 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:26.621369 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:26.633588 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.642577 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.642969 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.644249 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.644630 133632064819200 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0412 04:07:26.644913 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:26.645031 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:26.645100 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:26.645213 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.654650 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:26.673611 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.675642 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:26.685353 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:26.731069 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:26.731229 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:26.731331 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:26.731403 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.731600 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.732234 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.732497 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.733335 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.734773 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.740260 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.740961 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.741197 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:26.741286 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:26.741396 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.741800 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:26.742021 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:26.742139 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.747906 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.748171 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.755077 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.755338 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:26.755538 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:26.763153 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.768943 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.769221 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.769909 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.770106 133632064819200 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0412 04:07:26.770303 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:26.770382 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:26.770442 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:26.770537 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.776714 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:26.794909 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.797333 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:26.806468 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:26.847196 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:26.847510 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:26.847602 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:26.847675 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.847930 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.848579 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.848778 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.849666 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.851207 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.856655 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.857312 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.857530 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:26.857603 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:26.857711 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.858108 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:26.858339 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:26.858447 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.864237 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.864533 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.875089 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.875363 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:26.875565 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:26.882158 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.887930 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.888213 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.888963 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.889204 133632064819200 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0412 04:07:26.889435 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:26.889523 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:26.889590 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:26.889754 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.896748 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:26.915019 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.916785 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:26.924781 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:26.965749 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:26.965910 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:26.965991 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:26.966057 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.966263 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.966886 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.967079 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.967916 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.969417 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.976323 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.977061 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.977328 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:26.977412 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:26.977512 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.977909 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:26.978124 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:26.978232 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:26.984265 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:26.984562 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:26.991131 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.012519 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:27.012976 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:27.023841 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.033234 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.033632 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.034590 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.034861 133632064819200 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0412 04:07:27.035080 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:27.035166 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:27.035228 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:27.035389 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.041700 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:27.059993 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.061792 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:27.070376 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:27.115636 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:27.115791 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:27.115886 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:27.115957 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.116169 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.116808 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.117003 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.117843 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.119295 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.124785 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.125503 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.125730 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:27.125810 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:27.126052 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.126566 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:27.126776 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:27.126903 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.133409 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.133720 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.143351 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.143632 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:27.143842 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:27.150959 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.156757 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.157063 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.158125 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.158405 133632064819200 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0412 04:07:27.158602 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:27.158671 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:27.158733 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:27.158840 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.165637 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:27.184916 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.187690 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:27.196967 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:27.238672 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:27.238837 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:27.238927 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:27.238994 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.239199 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.239851 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.240045 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.240940 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.242412 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.247864 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.248553 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.248781 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:27.248863 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:27.248960 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.249402 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:27.249610 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:27.249720 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.256467 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.256817 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.264658 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.264937 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:27.265144 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:27.273002 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.281441 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.281821 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.282819 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.283092 133632064819200 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0412 04:07:27.283336 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:27.283426 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:27.283484 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:27.283586 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.290916 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:27.309233 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.311096 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:27.319910 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:27.360716 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:27.360919 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:27.361002 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:27.361074 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.361360 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.362065 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.362311 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.363161 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.364708 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.370548 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.371257 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.371532 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:27.371610 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:27.371717 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.372140 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:27.372430 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:27.372546 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.378570 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.378860 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.386106 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.386405 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:27.386611 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:27.393237 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.398765 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.399062 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.399848 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.400080 133632064819200 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0412 04:07:27.400300 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:27.400378 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:27.400439 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:27.400535 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.406414 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:27.426553 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.428665 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:27.437586 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:27.478629 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:27.478795 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:27.479004 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:27.479081 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.479291 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.480009 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.480219 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.481184 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.482705 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.493165 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.494079 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.494387 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:27.494464 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:27.494569 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.494982 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:27.495193 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:27.495349 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.501235 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.501571 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.508252 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.508513 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:27.508713 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:27.515519 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.521867 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.522190 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.522962 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.523192 133632064819200 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0412 04:07:27.523405 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:27.523478 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:27.523539 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:27.523637 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.530497 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:27.550232 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.552142 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:27.561142 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:27.602123 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:27.602303 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:27.602388 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:27.602466 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.602669 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.603341 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.603548 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.604456 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.605938 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.611573 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.612288 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.612531 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:27.612616 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:27.612713 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.613109 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:27.613340 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:27.613452 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.619453 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.619732 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.626414 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.626659 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:27.626880 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:27.634111 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.642229 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.642614 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.643573 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.643849 133632064819200 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0412 04:07:27.644069 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:07:27.644154 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:07:27.644214 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:07:27.644335 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.652760 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:07:27.709075 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.710961 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:07:27.720201 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:07:27.762994 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:07:27.763153 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:07:27.763227 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:07:27.763319 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.763513 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.764156 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.764390 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.765206 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.766704 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.772918 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.773804 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.774096 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:07:27.774180 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:07:27.774310 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.774740 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:07:27.774963 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:07:27.775095 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.781461 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.781762 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.789475 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.789838 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:07:27.790081 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:07:27.798336 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:07:27.805136 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.805495 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:07:27.806193 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:07:27.806454 133632064819200 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0412 04:07:27.810969 133632064819200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:00.259086 133632064819200 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000\n",
            "/usr/local/lib/python3.11/dist-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.\n",
            "  warnings.warn(\n",
            "I0412 04:08:01.038542 133632064819200 training_loop.py:409] No working directory specified.\n",
            "I0412 04:08:01.038703 133632064819200 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:\n",
            "I0412 04:08:01.039581 133632064819200 checkpoints.py:429] Restoring checkpoint from ag_ckpt_vocab/checkpoint_10999999\n",
            "I0412 04:08:08.961252 133632064819200 training_loop.py:447] Only restoring trainable parameters.\n",
            "I0412 04:08:08.962297 133632064819200 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.962470 133632064819200 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.962563 133632064819200 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.962642 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.962723 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.962812 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.962889 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.962970 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.963046 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.963122 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.963194 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.963268 133632064819200 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.963366 133632064819200 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.963442 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.963516 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.963588 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.963660 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.963733 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.963813 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.963889 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.963962 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.964034 133632064819200 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.964107 133632064819200 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.964181 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.964252 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.964338 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.964412 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.964485 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.964556 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.964627 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.964695 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.964766 133632064819200 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.964844 133632064819200 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.964920 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.964991 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.965063 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.965134 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.965205 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.965290 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.965363 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.965442 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.965517 133632064819200 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.965590 133632064819200 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.965661 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.965732 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.965814 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.965888 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.965961 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.966033 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.966107 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.966177 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.966250 133632064819200 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.966346 133632064819200 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.966420 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.966495 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.966566 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.966637 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.966708 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.966779 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.966859 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.966932 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.967003 133632064819200 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.967074 133632064819200 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.967145 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.967217 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.967304 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.967385 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.967457 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.967528 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.967600 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.967672 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.967743 133632064819200 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.967825 133632064819200 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.967896 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.967968 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.968039 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.968116 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.968195 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.968290 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.968371 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.968447 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.968523 133632064819200 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.968602 133632064819200 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.968680 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.968763 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.968861 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.968945 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.969029 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.969119 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.969207 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.969315 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.969408 133632064819200 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.969501 133632064819200 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.969604 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.969699 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.969799 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.969893 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.969974 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.970054 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.970142 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.970226 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.970329 133632064819200 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.970418 133632064819200 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.970501 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.970582 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.970667 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.970749 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.970851 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.970932 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.971015 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.971101 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.971186 133632064819200 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.971266 133632064819200 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256\n",
            "I0412 04:08:08.971374 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8\n",
            "I0412 04:08:08.971452 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.971531 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.971611 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.971698 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.971778 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304\n",
            "I0412 04:08:08.971870 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304\n",
            "I0412 04:08:08.971957 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576\n",
            "I0412 04:08:08.972042 133632064819200 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024\n",
            "I0412 04:08:08.972109 133632064819200 training_loop.py:725] Total parameters: 152072288\n",
            "I0412 04:08:08.972762 133632064819200 training_loop.py:739] Total state size: 0\n",
            "I0412 04:08:09.271502 133632064819200 training_loop.py:492] Training loop: creating task for mode beam_search\n",
            "I0412 04:08:09.271825 133632064819200 training_loop.py:685] Creating logging writer (train) for mode beam_search\n",
            "I0412 04:08:09.272747 133632064819200 training_loop.py:652] Compiling mode beam_search with jit.\n",
            "I0412 04:08:09.273430 133632064819200 training_loop.py:89] registering functions: dict_keys([])\n",
            "I0412 04:08:09.298374 133632064819200 graph.py:498] orthocenter\n",
            "I0412 04:08:09.298608 133632064819200 graph.py:499] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b ? perp a d b c\n",
            "I0412 04:08:09.330372 133632064819200 ddar.py:60] Depth 1/1000 time = 0.003911495208740234\n",
            "I0412 04:08:09.334620 133632064819200 ddar.py:60] Depth 2/1000 time = 0.004061222076416016\n",
            "I0412 04:08:09.335139 133632064819200 alphageometry.py:221] DD+AR failed to solve the problem.\n",
            "I0412 04:08:09.335263 133632064819200 alphageometry.py:539] Depth 0. There are 1 nodes to expand:\n",
            "I0412 04:08:09.335345 133632064819200 alphageometry.py:543] {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00\n",
            "I0412 04:08:09.335408 133632064819200 alphageometry.py:548] Decoding from {S} a : ; b : ; c : ; d : T a b c d 00 T a c b d 01 ? T a d b c {F1} x00\n",
            "I0412 04:08:09.424160 133632064819200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.424436 133632064819200 decoder_stack.py:316] dstack: scanning over 1 windows.\n",
            "I0412 04:08:09.424601 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.424685 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.424771 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.424868 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.424950 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.425034 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.425117 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.425198 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.425292 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.425383 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.425463 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.425544 133632064819200 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.\n",
            "I0412 04:08:09.425611 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:09.425682 133632064819200 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0412 04:08:09.425906 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:09.425981 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:09.426036 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:09.429732 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.436542 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:09.487370 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.489210 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:09.521603 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:09.572770 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:09.573164 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:09.573444 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:09.573610 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.574018 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.576492 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.576993 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.578458 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.583565 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.596959 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.599372 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.599981 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:09.600211 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:09.600424 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.601458 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:09.603124 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:09.603506 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:09.629950 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.630430 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:09.645139 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.645703 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:09.647571 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:09.676326 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:09.711854 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.712471 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:09.714962 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.715250 133632064819200 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0412 04:08:09.715498 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:09.715560 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:09.715606 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:09.722186 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.740048 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:09.775777 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.776548 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:09.780570 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:09.792677 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:09.792829 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:09.792908 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:09.792976 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.793140 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.793712 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.793860 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.794420 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.795724 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.798819 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.799411 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.799556 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:09.799631 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:09.799733 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.800084 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:09.800519 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:09.800596 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:09.808676 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.808882 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:09.812520 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.812722 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:09.813223 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:09.821123 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:09.828920 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.829155 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:09.829708 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.829897 133632064819200 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0412 04:08:09.830076 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:09.830143 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:09.830206 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:09.832549 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.836395 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:09.858819 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.859538 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:09.863643 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:09.875427 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:09.875569 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:09.875653 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:09.875721 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.875856 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.876446 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.876597 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.877139 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.878442 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.881951 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.882549 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.882699 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:09.882768 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:09.882865 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.883196 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:09.883622 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:09.883699 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:09.891016 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.891239 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:09.894997 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.895198 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:09.895693 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:09.903638 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:09.911143 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.911374 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:09.911881 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.912065 133632064819200 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0412 04:08:09.912234 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:09.912320 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:09.912384 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:09.914799 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.918701 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:09.940789 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.941550 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:09.944854 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:09.958015 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:09.958178 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:09.958258 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:09.958429 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.958589 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.959145 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.959365 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.959932 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.961219 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.964558 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.965134 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.965301 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:09.965375 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:09.965478 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.965815 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:09.966248 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:09.966354 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:09.974432 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.974640 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:09.978528 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:09.978703 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:09.979174 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:09.992715 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.000670 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.000913 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.001492 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.001686 133632064819200 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0412 04:08:10.001871 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.001939 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.001999 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.004334 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.008130 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.030599 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.031343 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.034750 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:10.046591 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.046746 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.046843 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.046913 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.047061 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.047651 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.047892 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.048559 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.049872 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.052965 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.053532 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.053676 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.053751 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.053861 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.054193 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.054630 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.054708 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.062752 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.062990 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.066856 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.067058 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.067595 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.075370 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.082473 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.082730 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.083419 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.083616 133632064819200 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0412 04:08:10.083791 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.083864 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.083925 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.086491 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.090481 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.112823 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.113610 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.117131 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:10.129452 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.129611 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.129691 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.129763 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.129915 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.130546 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.130701 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.131240 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.132595 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.136078 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.136725 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.136899 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.136975 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.137078 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.137468 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.137903 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.137982 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.145317 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.145568 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.149523 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.149740 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.150257 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.158164 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.165973 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.166215 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.166771 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.166972 133632064819200 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0412 04:08:10.167153 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.167220 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.167533 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.170197 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.174501 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.203205 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.203953 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.207444 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:10.220132 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.220302 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.220384 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.220452 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.220594 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.221162 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.221328 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.221863 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.223228 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.226605 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.227228 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.227418 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.227488 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.227594 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.227960 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.228435 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.228514 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.236144 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.236434 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.240171 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.240391 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.240869 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.248540 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.255538 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.255762 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.256305 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.256506 133632064819200 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0412 04:08:10.256856 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.256980 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.257043 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.259823 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.264855 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.291803 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.292610 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.296037 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:10.307846 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.308000 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.308109 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.308177 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.308347 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.309072 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.309295 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.309997 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.311911 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.317050 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.317916 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.318171 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.318260 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.318395 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.318841 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.319441 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.319540 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.331633 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.331932 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.338116 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.338433 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.338984 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.346983 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.354161 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.354411 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.354963 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.355161 133632064819200 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0412 04:08:10.355368 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.355439 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.355501 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.357935 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.361792 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.384843 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.385626 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.389400 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:10.401432 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.401581 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.401666 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.401738 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.401882 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.402482 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.402635 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.403182 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.404472 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.407627 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.408221 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.408410 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.408482 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.408590 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.408927 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.409372 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.409450 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.416778 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.416999 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.420845 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.421023 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.421530 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.429201 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.436668 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.436894 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.437470 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.437660 133632064819200 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0412 04:08:10.437836 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.437903 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.437964 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.440364 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.444302 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.466880 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.467631 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.471511 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:10.483603 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.483754 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.483850 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.483918 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.484058 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.484684 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.484849 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.485743 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.488022 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.494706 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.495547 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.495743 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.495812 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.495920 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.496335 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.496921 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.497019 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.505899 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.506127 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.509934 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.510131 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.510659 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.518366 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.525773 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.526012 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.526579 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.526767 133632064819200 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0412 04:08:10.526951 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.527019 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.527080 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.529462 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.533452 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.556970 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.557715 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.561172 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:10.573120 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.573291 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.573379 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.573447 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.573585 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.574140 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.574307 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.574839 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.576186 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.579535 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.580130 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.580295 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.580370 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.580471 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.580804 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.581250 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.581343 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.589454 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.589684 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.593687 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.593883 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.594435 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.602235 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.609360 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.609601 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.610176 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.610383 133632064819200 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0412 04:08:10.610564 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.610632 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.610693 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.613059 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.616944 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.638952 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.639664 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.643238 133632064819200 transformer_layer.py:213] tlayer: windowed attention.\n",
            "I0412 04:08:10.655671 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.655824 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.655908 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.655979 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.656134 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.656723 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.656877 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.657453 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.658704 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.661788 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.662381 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.662531 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.662612 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.662716 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.663049 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.663499 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.663576 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.670645 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.670846 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.674582 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.674758 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.675430 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.683395 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.691127 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.691416 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.692021 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.692558 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.692678 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.692764 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.692846 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.692939 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.693022 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.693096 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.693202 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.693281 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.693353 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.693425 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.693493 133632064819200 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.\n",
            "I0412 04:08:10.693563 133632064819200 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0412 04:08:10.700663 133632064819200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=1/0)>\n",
            "I0412 04:08:10.852382 133632064819200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.852651 133632064819200 decoder_stack.py:333] dstack: autoregressive generator.\n",
            "I0412 04:08:10.852759 133632064819200 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0412 04:08:10.853021 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.853165 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.853288 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.853499 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.857024 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.870930 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.871668 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.875185 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:10.897301 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.897532 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.897673 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.897893 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.898118 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.898965 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.899202 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.900111 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.902223 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.908764 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.909754 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.909971 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.910105 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.910380 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.910979 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.911222 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.911318 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.916451 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.916653 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.920407 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.920626 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:10.920842 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:10.926206 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.931220 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.931447 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:10.932050 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.932256 133632064819200 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0412 04:08:10.932533 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:10.932652 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:10.932765 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:10.932924 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.936688 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:10.951115 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.951865 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:10.955817 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:10.981092 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:10.981340 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:10.981501 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:10.981768 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.982054 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.982836 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.983005 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.983736 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.985327 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.989537 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.990302 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.990496 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:10.990619 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:10.990827 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.991441 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:10.991682 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:10.991757 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:10.997214 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:10.997444 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.001076 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.001330 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.001563 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.007293 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.011981 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.012198 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.012733 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.012940 133632064819200 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0412 04:08:11.013183 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.013319 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.013430 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.013588 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.016781 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.030549 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.031252 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.034744 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.055104 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.055333 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.055493 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.055667 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.055843 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.056562 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.056763 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.057495 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.058946 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.062969 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.063758 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.063954 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.064075 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.064304 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.064893 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.065155 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.065239 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.070388 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.070597 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.074374 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.074612 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.074827 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.080178 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.084962 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.085184 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.085723 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.085935 133632064819200 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0412 04:08:11.086182 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.086322 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.086430 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.086587 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.089739 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.107209 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.108683 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.114472 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.136843 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.137061 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.137216 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.137417 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.137618 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.138363 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.138551 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.139307 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.141000 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.145034 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.145814 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.146019 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.146149 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.146411 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.147036 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.147301 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.147385 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.153551 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.153795 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.157552 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.157819 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.158046 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.163713 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.168524 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.168752 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.169303 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.169528 133632064819200 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0412 04:08:11.169799 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.169924 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.170029 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.170198 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.173449 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.189220 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.189971 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.193737 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.215470 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.215679 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.215839 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.216038 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.216216 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.216992 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.217177 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.217969 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.219591 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.223528 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.224298 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.224519 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.224644 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.224867 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.225489 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.225717 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.225793 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.231078 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.231313 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.235000 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.235250 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.235499 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.240948 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.245667 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.245901 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.246443 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.246644 133632064819200 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0412 04:08:11.246901 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.247018 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.247169 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.247371 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.250566 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.267177 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.268091 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.272614 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.295183 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.295451 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.295611 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.295814 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.295991 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.296734 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.296915 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.297661 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.299167 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.303183 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.303924 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.304124 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.304305 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.304599 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.305444 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.305960 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.306060 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.313615 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.313883 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.319343 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.319622 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.319856 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.325376 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.330062 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.330306 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.330841 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.331051 133632064819200 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0412 04:08:11.331338 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.331466 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.331579 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.331752 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.335064 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.357578 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.358705 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.364745 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.389144 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.389378 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.389532 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.389710 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.389886 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.390590 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.390772 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.391508 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.392995 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.396943 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.397669 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.397857 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.398136 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.398448 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.399134 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.399407 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.399489 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.404707 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.404916 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.409200 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.409498 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.409733 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.415495 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.420378 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.420594 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.421137 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.421365 133632064819200 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0412 04:08:11.421622 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.421740 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.421850 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.422018 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.425621 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.439533 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.440245 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.443773 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.465387 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.465611 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.465764 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.466093 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.466305 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.467032 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.467217 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.467954 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.469474 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.473443 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.474192 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.474421 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.474546 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.474765 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.475383 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.475609 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.475684 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.480706 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.480927 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.484721 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.484983 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.485208 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.490813 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.495511 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.495739 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.496335 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.496565 133632064819200 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0412 04:08:11.497021 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.497149 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.497264 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.497466 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.500755 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.519785 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.520590 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.524236 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.546902 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.547117 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.547287 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.547490 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.547663 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.548391 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.548575 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.549297 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.550885 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.554736 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.555477 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.555657 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.555786 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.555995 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.556595 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.556850 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.556926 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.562140 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.562352 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.565907 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.566132 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.566383 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.571894 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.576834 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.577070 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.577636 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.577853 133632064819200 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0412 04:08:11.578120 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.578245 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.578398 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.578572 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.581888 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.596307 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.597081 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.600885 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.623190 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.623426 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.623578 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.623749 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.623934 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.624646 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.624833 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.625569 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.627115 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.631066 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.631839 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.632055 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.632182 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.632427 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.633065 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.633350 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.633431 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.638808 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.639040 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.642812 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.643072 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.643329 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.649070 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.654055 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.654310 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.654861 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.655078 133632064819200 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0412 04:08:11.655374 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.655502 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.655615 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.655802 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.659082 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.672892 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.673631 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.677415 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.699508 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.699947 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.700239 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.700465 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.700664 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.701421 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.701606 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.702372 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.703951 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.707961 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.708916 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.709161 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.709326 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.709602 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.710445 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.710723 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.710813 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.719850 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.720144 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.725356 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.725687 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.725969 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.734029 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.738935 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.739175 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.739715 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.739929 133632064819200 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0412 04:08:11.740189 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.740327 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.740438 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.740598 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.743837 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.757509 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.758396 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.762338 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:11.784143 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:11.784379 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:11.784533 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:11.784717 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.784900 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.785610 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.785789 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.786543 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.788048 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.792518 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.793304 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.793502 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:11.793622 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:11.793836 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.794444 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:11.794679 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:11.794757 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.799740 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.799950 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.803592 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.803824 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:11.804040 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:11.809424 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:11.814372 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.814573 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:11.815067 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.815268 133632064819200 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0412 04:08:11.821895 133632064819200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=3/0)>\n",
            "I0412 04:08:11.926574 133632064819200 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:11.926797 133632064819200 decoder_stack.py:333] dstack: autoregressive generator.\n",
            "I0412 04:08:11.926894 133632064819200 decoder_stack.py:224] dstack: ---- Layer 0 ----\n",
            "I0412 04:08:11.927056 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:11.927118 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:11.927175 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:11.927266 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:11.933237 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:11.950976 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:11.952695 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:11.959917 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.000062 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.000219 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.000355 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.000427 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.000684 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.001329 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.001539 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.002384 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.003839 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.010595 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.011621 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.011936 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.012026 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.012220 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.012710 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.012953 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.013119 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.020534 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.020903 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.029808 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.030059 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.030249 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.036798 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.042099 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.042395 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.043160 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.043387 133632064819200 decoder_stack.py:224] dstack: ---- Layer 1 ----\n",
            "I0412 04:08:12.043565 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.043633 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.043694 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.043792 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.049703 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:12.067441 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.069844 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:12.078648 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.124999 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.125165 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.125248 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.125339 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.125557 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.126191 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.126459 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.127351 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.128850 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.134932 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.135679 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.135949 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.136032 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.136141 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.136574 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.136788 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.137077 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.144431 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.144742 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.152650 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.152899 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.153110 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.159879 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.165483 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.165782 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.166539 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.166752 133632064819200 decoder_stack.py:224] dstack: ---- Layer 2 ----\n",
            "I0412 04:08:12.166936 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.167006 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.167064 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.167161 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.173074 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:12.192634 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.194308 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:12.201654 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.240989 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.241146 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.241230 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.241328 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.241537 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.242166 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.242404 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.243255 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.244822 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.250268 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.250980 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.251226 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.251320 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.251432 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.251833 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.252031 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.252171 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.257966 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.258265 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.266076 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.266343 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.266549 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.273252 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.279014 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.279342 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.280078 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.280334 133632064819200 decoder_stack.py:224] dstack: ---- Layer 3 ----\n",
            "I0412 04:08:12.280524 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.280591 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.280652 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.280751 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.286561 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:12.305450 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.307064 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:12.314374 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.359252 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.359419 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.359499 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.359572 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.359790 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.360504 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.360721 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.361641 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.363184 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.369049 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.369795 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.370070 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.370158 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.370264 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.370754 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.371007 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.371168 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.380459 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.380863 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.391836 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.392169 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.392453 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.402426 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.408241 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.408603 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.409401 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.409618 133632064819200 decoder_stack.py:224] dstack: ---- Layer 4 ----\n",
            "I0412 04:08:12.409802 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.409878 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.409940 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.410037 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.415960 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:12.435477 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.437193 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:12.444300 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.482648 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.482813 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.482901 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.482972 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.483203 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.483871 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.484100 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.485105 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.486741 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.492435 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.493162 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.493463 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.493548 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.493656 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.494063 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.494335 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.494484 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.500377 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.500675 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.508754 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.509038 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.509353 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.516809 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.522543 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.522869 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.523662 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.523884 133632064819200 decoder_stack.py:224] dstack: ---- Layer 5 ----\n",
            "I0412 04:08:12.524073 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.524143 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.524203 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.524334 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.533205 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:12.555371 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.557106 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:12.564615 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.602167 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.602337 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.602429 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.602503 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.602723 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.603447 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.603670 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.604571 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.606063 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.611793 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.612544 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.612805 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.612886 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.612999 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.613502 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.613712 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.613857 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.619714 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.620009 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.627497 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.627737 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.627935 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.634739 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.640304 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.640609 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.641381 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.641619 133632064819200 decoder_stack.py:224] dstack: ---- Layer 6 ----\n",
            "I0412 04:08:12.641804 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.641881 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.641943 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.642041 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.647880 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:12.666981 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.668869 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:12.676202 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.715770 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.715940 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.716020 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.716089 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.716342 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.717005 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.717220 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.718173 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.719693 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.725259 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.726015 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.726282 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.726370 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.726477 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.726972 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.727222 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.727413 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.735307 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.735702 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.743804 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.744056 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.744261 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.750900 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.756581 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.756914 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.757665 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.757894 133632064819200 decoder_stack.py:224] dstack: ---- Layer 7 ----\n",
            "I0412 04:08:12.758078 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.758146 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.758208 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.758322 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.764617 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:12.783179 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.785104 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:12.792646 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.832094 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.832315 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.832422 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.832540 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.833004 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.834861 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.835134 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.836343 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.838587 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.845866 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.846601 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.846863 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.846939 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.847041 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.847481 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.847679 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.847830 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.853710 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.854011 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.861262 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.861507 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.861701 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.868128 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.873986 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.874286 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.875002 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.875208 133632064819200 decoder_stack.py:224] dstack: ---- Layer 8 ----\n",
            "I0412 04:08:12.875409 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.875478 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.875539 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.875635 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.881308 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:12.899514 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.901198 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:12.908387 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:12.948717 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:12.948895 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:12.948994 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:12.949069 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.949301 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.949941 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.950150 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.951054 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.952634 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.958069 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.958742 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.958989 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:12.959070 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:12.959178 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.959621 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:12.959822 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:12.959963 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.966316 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.966683 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.975745 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.976153 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:12.976387 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:12.986488 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:12.994091 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.994448 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:12.995223 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:12.995490 133632064819200 decoder_stack.py:224] dstack: ---- Layer 9 ----\n",
            "I0412 04:08:12.995673 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:12.995741 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:12.995807 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:12.995902 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.002214 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:13.020862 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.022654 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:13.029685 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:13.074698 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:13.074855 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:13.074934 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:13.075006 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.075221 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.075842 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.076045 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.076901 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.078422 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.083786 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.084504 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.084749 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:13.084829 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:13.084932 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.085349 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:13.085546 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:13.085689 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:13.091358 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.091645 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:13.098621 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.098847 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:13.099043 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:13.105392 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:13.110896 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.111188 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:13.111908 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.112119 133632064819200 decoder_stack.py:224] dstack: ---- Layer 10 ----\n",
            "I0412 04:08:13.112322 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:13.112399 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:13.112460 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:13.112555 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.118103 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:13.135782 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.137538 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:13.144624 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:13.181144 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:13.181336 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:13.181427 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:13.181499 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.181709 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.182405 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.182611 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.183465 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.185145 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.191758 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.192495 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.192744 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:13.192834 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:13.192940 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.193398 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:13.193616 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:13.193758 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:13.199439 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.199731 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:13.206960 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.207205 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:13.207430 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:13.213888 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:13.219246 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.219570 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:13.220325 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.220541 133632064819200 decoder_stack.py:224] dstack: ---- Layer 11 ----\n",
            "I0412 04:08:13.220717 133632064819200 transformer_layer.py:154] tlayer: recurrent = False\n",
            "I0412 04:08:13.220782 133632064819200 transformer_layer.py:155] tlayer: compute_importance = False\n",
            "I0412 04:08:13.221097 133632064819200 transformer_layer.py:161] tlayer: compute keys,values,queries.\n",
            "I0412 04:08:13.221253 133632064819200 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.228012 133632064819200 transformer_base.py:161] kvq: pre_attn dropout.\n",
            "I0412 04:08:13.258027 133632064819200 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.260133 133632064819200 transformer_base.py:194] kvq: normalize keys, queries.\n",
            "I0412 04:08:13.268397 133632064819200 transformer_layer.py:169] tlayer: using autoregressive decoder.\n",
            "I0412 04:08:13.311034 133632064819200 transformer_layer.py:299] tlayer: num_windows = 1.\n",
            "I0412 04:08:13.311218 133632064819200 attention.py:418] Single window, no scan.\n",
            "I0412 04:08:13.311329 133632064819200 transformer_layer.py:389] tlayer: self-attention.\n",
            "I0412 04:08:13.311410 133632064819200 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[2,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.311671 133632064819200 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.312549 133632064819200 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.312793 133632064819200 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.313764 133632064819200 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.315542 133632064819200 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.321613 133632064819200 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[2,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.322369 133632064819200 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.322622 133632064819200 transformer_layer.py:468] tlayer: End windows.\n",
            "I0412 04:08:13.322703 133632064819200 transformer_layer.py:472] tlayer: final FFN.\n",
            "I0412 04:08:13.322823 133632064819200 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[2,1,8,128])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.323229 133632064819200 transformer_base.py:410] tbase: post-attention MLP.\n",
            "I0412 04:08:13.323458 133632064819200 nn_components.py:325] mlp: activation = None\n",
            "I0412 04:08:13.323629 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:13.329756 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.330041 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:13.338643 133632064819200 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.338920 133632064819200 transformer_base.py:443] tbase: final FFN\n",
            "I0412 04:08:13.339147 133632064819200 nn_components.py:320] mlp: hidden 4096, relu\n",
            "I0412 04:08:13.346454 133632064819200 nn_components.py:329] mlp: final activation = None\n",
            "I0412 04:08:13.352507 133632064819200 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.352854 133632064819200 nn_components.py:261] mlp: residual\n",
            "I0412 04:08:13.353720 133632064819200 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:08:13.353987 133632064819200 decoder_stack.py:344] dstack: Final layernorm.\n",
            "I0412 04:08:13.358342 133632064819200 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[2,1,1024])>with<DynamicJaxprTrace(level=2/0)>\n",
            "I0412 04:09:55.828562 133632064819200 alphageometry.py:565] LM output (score=-1.123075): \"e : C a c e 02 C b d e 03 ;\"\n",
            "I0412 04:09:55.828800 133632064819200 alphageometry.py:566] Translation: \"e = on_line e a c, on_line e b d\"\n",
            "\n",
            "I0412 04:09:55.828888 133632064819200 alphageometry.py:575] Solving: \"a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c\"\n",
            "I0412 04:09:55.829092 133632064819200 graph.py:498] \n",
            "I0412 04:09:55.829174 133632064819200 graph.py:499] a b c = triangle a b c; d = on_tline d b a c, on_tline d c a b; e = on_line e a c, on_line e b d ? perp a d b c\n",
            "I0412 04:09:55.867662 133632064819200 ddar.py:60] Depth 1/1000 time = 0.031670331954956055\n",
            "I0412 04:09:55.909705 133632064819200 ddar.py:60] Depth 2/1000 time = 0.04179120063781738\n",
            "I0412 04:09:55.955935 133632064819200 ddar.py:60] Depth 3/1000 time = 0.0459742546081543\n",
            "I0412 04:09:55.961939 133632064819200 alphageometry.py:191] \n",
            "==========================\n",
            " * From theorem premises:\n",
            "A B C D : Points\n",
            "BD ⟂ AC [00]\n",
            "CD ⟂ AB [01]\n",
            "\n",
            " * Auxiliary Constructions:\n",
            "E : Points\n",
            "E,A,C are collinear [02]\n",
            "B,D,E are collinear [03]\n",
            "\n",
            " * Proof steps:\n",
            "001. E,A,C are collinear [02] & E,B,D are collinear [03] & BD ⟂ AC [00] ⇒  ∠AEB = ∠DEC [04]\n",
            "002. E,A,C are collinear [02] & E,B,D are collinear [03] & BD ⟂ AC [00] ⇒  ∠AED = ∠BEC [05]\n",
            "003. E,A,C are collinear [02] & BD ⟂ AC [00] ⇒  BD ⟂ EA [06]\n",
            "004. CD ⟂ AB [01] & BD ⟂ EA [06] ⇒  ∠ABD = ∠(CD-EA) [07]\n",
            "005. B,D,E are collinear [03] & E,C,A are collinear [02] & ∠ABD = ∠(CD-EA) [07] ⇒  ∠ABE = ∠DCE [08]\n",
            "006. ∠AEB = ∠DEC [04] & ∠ABE = ∠DCE [08] (Similar Triangles)⇒  EA:ED = BE:EC [09]\n",
            "007. EA:ED = BE:EC [09] & ∠AED = ∠BEC [05] (Similar Triangles)⇒  ∠ADE = ∠BCE [10]\n",
            "008. EA:ED = BE:EC [09] & ∠AED = ∠BEC [05] (Similar Triangles)⇒  ∠EAD = ∠EBC [11]\n",
            "009. ∠ADE = ∠BCE [10] & B,D,E are collinear [03] & E,A,C are collinear [02] & ∠EAD = ∠EBC [11] ⇒  AD ⟂ BC\n",
            "==========================\n",
            "\n",
            "I0412 04:09:56.018625 133632064819200 alphageometry.py:581] Solved.\n"
          ]
        }
      ]
    }
  ]
}